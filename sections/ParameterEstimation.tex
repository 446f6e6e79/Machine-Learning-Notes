\section{Parameter Estimation}
\paragraph{Setting}
In order to understand what we will be doing in this section, we need to first introduce the setting of 
the environment we are working in. We have:
\begin{itemize}
    \item A \textbf{collection of data} sampled from a \textbf{probability distribution} $p(x,y)$;
    \item The \textbf{probability distribution} $p(x,y)$ is \textbf{known}, but the parameters $\theta$ 
    of the distribution are \textbf{unknown};
    \item There is a \textbf{training set} $\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \ldots, (x_N, y_N)\}$ of $N$ examples,
    sampled \textbf{i.i.d.} from the distribution $p(x,y)$;
\end{itemize}
\begin{figure}[H]
\definition{I.I.D.}{
    \textit{Independent and Identically Distributed} (i.i.d.) refers to a set of random variables
    that are all drawn from the same probability distribution and are mutually independent.
}
\end{figure}
The \textbf{task} is to estimate the parameters $\theta$ of the distribution $p(x,y)$ using the training set $\mathcal{D}$,
in order to make predictions on new examples.

\paragraph{Multiclass classification setting:}
\begin{itemize}
    \item The training set $\mathcal{D}$ can be divided into $c$ subsets $\mathcal{D}_1,\dots,\mathcal{D}_c$,
    where each $\mathcal{D}_i = \{\boldsymbol{x}_1,\dots,\boldsymbol{x}_{n}\}$ contains examples of class $y_i$;
    \item For any new example $\boldsymbol{x}$, we want to be able to compute the posterior probability for each class $y_i$
    (what's the probability of $y_i$ given $\boldsymbol{x}$) using Bayes' theorem:
    \begin{equation}\label{eq:multiclass-bayes}
        p(y_i|\boldsymbol{x}, \mathcal{D}) = \frac{p(\boldsymbol{x}|y_i, \mathcal{D})p(y_i|\mathcal{D})}{p(\boldsymbol{x}|\mathcal{D})}
    \end{equation}
\end{itemize}
\begin{figure}[H]

\definition{A posteriori probability}{
    The probability of a class $y_i$ given the features $\boldsymbol{x}$ and the training set $\mathcal{D}$, 
    denoted as $p(y_i|\boldsymbol{x}, \mathcal{D})$, is called the posterior probability. 
    It represents the probability of $x$ belonging to class $y_i$ after observing the data $\mathcal{D}$.
}
\end{figure}
\paragraph{Simplification:}
Starting from the equation \ref{eq:multiclass-bayes}, we can simplify it by making some assumptions:
\begin{itemize}
    \item We assume $\boldsymbol{x}$ is independent of $\mathcal{D}_j (j \neq i)$, so:
    \begin{equation}
        \label{eq:independence-assumption}
        P(y_i|\boldsymbol{x}, \mathcal{D}) = \frac{p(\boldsymbol{x}|y_i, \mathcal{D}_i)p(y_i|\mathcal{D})}{p(\boldsymbol{x}|\mathcal{D})}
    \end{equation}
    \item Without additional knowledge, $P(y_i|\mathcal{D})$ can be computed as the fraction of examples in class $y_i$ in the training set:
    \[P(y_i|\mathcal{D}) = \frac{|\mathcal{D}_i|}{|\mathcal{D}|}\]
    \item The denominator $p(\boldsymbol{x}|\mathcal{D})$ can be computed using the \textbf{law of total probability}:
    \[p(\boldsymbol{x}|\mathcal{D}) = \sum_{i=1}^{c} p(\boldsymbol{x}|y_i, \mathcal{D})p(y_i|\mathcal{D})\]
\end{itemize}
At this point the only factor of equation \ref{eq:independence-assumption} that remains unknown is $p(\boldsymbol{x}|y_i, \mathcal{D}_i)$.
In order to estimate it, we need to estimate the class dependent parameter $\theta_i$ of the distribution.
\\This can be done in two ways:
\begin{enumerate}
    \item \textbf{Maximum Likelihood Estimation} (MLE): we assume that the parameter $\theta_i$ is a fixed but unknown values.
     We estimate it by maximizing the likelihood of the observed data $\mathcal{D}_i$. Obtained values are used to compute probabilities of new examples:
     \[p(\boldsymbol{x}|y_i, \mathcal{D}_i) \approx p(\boldsymbol{x}|{\theta}_i)\]
    \item \textbf{Bayesian estimation}: we assume that the parameter $\theta_i$ is a random variable with a prior distribution $p(\theta_i)$.
    Observed data turns the prior into a posterior distribution.
    The prediction for new examples is obtained by integrating over all possible values of $\theta_i$:
    \[p(\boldsymbol{x}|y_i, \mathcal{D}_i) = \int_{\theta_i} p(\boldsymbol{x},\theta_i|y_i, \mathcal{D}_i) d\theta_i\]
\end{enumerate}

\subsection{Maximum Likelihood Estimation}
Maximum Likelihood Estimation (\textbf{MLE}) is a method used to estimate the parameters of a statistical model.
Given a set of training data $\mathcal{D} = \{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n\}$, the estimation of 
the parameter $\theta$ is done by finding the parameters with the highest likelihood of generating the observed data.
\begin{equation}
    \theta_i^* = \text{argmax}_{\theta} p(\theta_i|\mathcal{D}_i, y_i) = \text{argmax}_{\theta_i} \frac{p(\mathcal{D}_i,y_i | \theta_i) p(\theta_i)}{p(\mathcal{D}_i,y_i)}
\end{equation}
Since the denominator is independent of $\theta_i$, we can ignore it for the purpose of maximization:
\begin{equation*}
    \theta_i^* = \text{argmax}_{\theta_i} p(\mathcal{D}_i,y_i | \theta_i) p(\theta_i)
\end{equation*}
This equation assumes that a prior distribution $p(\theta_i)$ is given.
\\\\The most common form of MLE, which doesn't take into account the prior, but only the training samples, is given by:
\begin{equation}
    \theta^* = \text{argmax}_{\theta} p(\mathcal{D}, y|\theta)
\end{equation}
Since the examples are \textbf{i.i.d.}, the joint probability over $D$ can be expressed as the product of the individual probabilities:
\begin{equation}\label{eq:mle-likelihood}
    \theta^* = \text{argmax}_{\theta} \prod_{j=1}^{n} p(\boldsymbol{x}_j|\theta)
\end{equation}
To find the value of $\theta$ that maximizes the likelihood, we would have to compute the derivative of the likelihood function and set it to zero.
This is often difficult to compute, so instead we work with the \textbf{log-likelihood}.

\subsubsection{Maximizing the log-likelihood}
The \textbf{log-likelihood} is defined as the logarithm of the likelihood function (equation \ref{eq:mle-likelihood}):
\begin{equation}
    \theta^* = \text{argmax}_{\theta} \sum_{j = 1}^{n} \ln p(\boldsymbol{x}_j|\theta)
\end{equation}
The logarithm is a \textbf{monotonically increasing function}, so maximizing the log-likelihood is equivalent to maximizing the likelihood itself.
The logarithm transforms the product of probabilities into a sum, making it easier to compute the derivative.
\\
In order to \textbf{find the maximum}, we compute the derivative of the log-likelihood wrt $\theta$ and set it to zero:
\begin{equation}
    \nabla_{\theta} \sum_{j = 1}^{n} \ln p(\boldsymbol{x}_j|\theta) = 0
\end{equation}
Solving this equation will give us a local or global maximum of the log-likelihood function, depending on the shape of the function.

\subsubsection{Example: MLE for Gaussian distribution}
Assuming that the data is generated from a Gaussian distribution, we can estimate the parameters $\mu$ and $\sigma^2$ using MLE.
\\First, the probability density function of a Gaussian distribution is given by:
\[
    p(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\]
Considering its logarithm, we have:
\[
    \sum_{j = 1}^{n} \ln p(\boldsymbol{x}_j|\theta) = \sum_{j = 1}^{n} \ln \left[\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(\boldsymbol{x}_j - \mu)^2}{2\sigma^2}\right)\right] =
    \sum_{j = 1}^{n} \ln(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{(\boldsymbol{x}_j - \mu)^2}{2\sigma^2}
\]
\paragraph{Estimating $\mu$:}
\[
    \frac{\partial \log p(\boldsymbol{x}|\theta)}{\partial \mu} = \frac{\partial}{\partial \mu}  \sum_{j = 1}^{n} \left[\ln(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{(\boldsymbol{x}_j - \mu)^2}{2\sigma^2} \right]
\]
The derivative of the first term is zero, so we only need to compute the derivative of the second term, which is:
\[
    = \sum_{j = 1}^{n} -\frac{1}{2\sigma^2} \cdot 2(\boldsymbol{x}_j - \mu)(-1) = 
    \sum_{j = 1}^{n} \frac{(\boldsymbol{x}_j - \mu)}{\sigma^2}
\]
Setting this quantity to zero, we have:
\[
    \sum_{j = 1}^{n} \frac{(\boldsymbol{x}_j - \mu)}{\sigma^2} = 0
\]
\[
    \sum_{j = 1}^{n} (\boldsymbol{x}_j - \mu) = 0
\]
\[
    \sum_{j = 1}^{n} \boldsymbol{x}_j - \sum_{j = 1}^{n}\mu = 0 \implies \sum_{j = 1}^{n} \boldsymbol{x}_j - n\mu = 0
\]
\[
    \mu = \frac{1}{n} \sum_{j = 1}^{n} \boldsymbol{x}_j
\]
Which is the \textbf{sample mean} of the data.
\paragraph{Estimating $\sigma$:}
Repeating the same process for $\sigma$, we have:
\[
    \frac{\partial \log p(\boldsymbol{x}|\theta)}{\partial \sigma} = \frac{\partial}{\partial \sigma}  \sum_{j = 1}^{n} \left[ \ln(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{(\boldsymbol{x}_j - \mu)^2}{2\sigma^2} \right]
\]
Focusing on the first term, we rewrite it as:
\[
    \ln(\frac{1}{\sqrt{2\pi\sigma^2}}) = \ln([2\pi\sigma^2]^{-\frac{1}{2}}) = -\frac{1}{2} \ln(2\pi\sigma^2)
\]
Using the properties of logarithms, we can separate the terms:
\[
    = -\frac{1}{2} \ln(2\pi) - \frac{1}{2} \ln(\sigma^2)
\]
Since the term $-\frac{1}{2} \ln(2\pi)$ is independent of $\sigma$, its derivative is zero.
We only need to compute the derivative of:
\[
    = \frac{\partial}{\partial \sigma}\sum_{j = 1}^{n} \left[-\frac{1}{2} \ln(\sigma^2) - \frac{(\boldsymbol{x}_j - \mu)^2}{2}(\sigma^{-2})\right]
\]
\[
    = \sum_{j = 1}^{n} -\frac{1}{\sigma} + \frac{(\boldsymbol{x}_j - \mu)^2}{\sigma^3}
\]
if we set the quantity to zero, we have:
\[
    \sum_{j = 1}^{n} -\frac{1}{\sigma} + \frac{(\boldsymbol{x}_j - \mu)^2}{\sigma^3} = 0
\]
Multiplying by $\sigma^3$, we have:
\[
    \sum_{j = 1}^{n} -\sigma^2 + (\boldsymbol{x}_j - \mu)^2 = 0
\]
\[
    \sigma^2 = \frac{1}{n} \sum_{j = 1}^{n} (\boldsymbol{x}_j - \mu)^2
\]
Which is the \textbf{sample variance} of the data.
\paragraph{Conclusion:}
By this two derivations, we have found that the maximum likelihood estimators for 
the parameters of a Gaussian distribution are simply their estimates over the samples:
\begin{itemize}
    \item Gaussian mean $\mu$ is estimated by the sample mean;
    \item Gaussian variance $\sigma^2$ is estimated by the sample variance.
\end{itemize}

\subsection{Bayesian Estimation}
With this approach we focus on the estimation of the parameters of our distribution $p(\boldsymbol{x}, y)$ a priori, 
meaning that we update our knowledge about the parameters $\theta$ based on the observed data.
\\We start by assuming that the parameters $\theta_i$ are \textbf{random variables} with a known \textbf{prior distribution}.
\\Predictions for new examples are obtained by integrating over all possible values of $\theta_i$:
\[
    p(\boldsymbol{x}|y_i, \mathcal{D}_i) = \int_{\theta_i} p(\boldsymbol{x},\theta_i|y_i, \mathcal{D}_i) d\theta_i
\]
Since probability of $\boldsymbol{x}$ given each class $y_i$ is independent of the other classes, we can simplify the equation as:
\[
    p(\boldsymbol{x}|\mathcal{D}) = \int_{\theta} p(\boldsymbol{x},\theta| \mathcal{D}) d\theta
\]
Using the definition of conditional probability, we can rewrite the equation as:
\[
    p(\boldsymbol{x}|\mathcal{D}) = \int_{\theta} p(\boldsymbol{x}|\theta, \mathcal{D}) p(\theta|\mathcal{D}) d\theta
\]
In computing $p(\boldsymbol{x}|\theta, \mathcal{D})$ we can assume that $\boldsymbol{x}$ is independent of $\mathcal{D}$ given $\theta$, 
since the parameter $\theta$ captures all the information about the distribution.
Thus, we have:
\[
    p(\boldsymbol{x}|\mathcal{D}) = \int_{\theta} p(\boldsymbol{x}|\theta) p(\theta|\mathcal{D}) d\theta
\]
\begin{itemize}
    \item The term $p(\boldsymbol{x}|\theta)$ can be easily computed since both the distribution and the initial parameters $\theta$ are known;
    \item The term $p(\theta|\mathcal{D})$ is the \textbf{posterior distribution} of the parameter $\theta$ given the data $\mathcal{D}$.
\end{itemize}
Using Bayes' theorem, we can express the posterior distribution as:
\begin{equation} \label{eq:bayes-posterior}
    p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta) p(\theta)}{p(\mathcal{D})}
\end{equation}
Where $p(\mathcal{D})$ is a constant independent of $\theta$, so we can ignore it for the purpose of estimation.

\subsubsection{Example: Bayesian estimation for Gaussian distribution}
\paragraph{Unknown $\mu$, known $\sigma$:}
Assuming that the data is drawn from a Gaussian distribution $p(\boldsymbol{x}|\mu) \sim \mathcal{N}(\mu, \sigma^2)$.
\\The prior distribution for $\theta = \mu$ is also a Gaussian distribution: $p(\mu) \sim \mathcal{N}(\mu_0, \sigma_0^2)$.
Where:
\begin{itemize}
    \item $\mu_0$ is the prior mean (our initial guess for the mean of the data);
    \item $\sigma_0^2$ is the prior variance (our initial uncertainty about the mean of the data).
\end{itemize}
Using Bayes' theorem, we can compute the posterior distribution for $\mu$ given the data $\mathcal{D}$.
Starting from the equation \ref{eq:bayes-posterior} we have:
\[
    p(\mu|\mathcal{D}) = \frac{p(\mathcal{D}|\mu) p(\mu)}{p(\mathcal{D})}
\]
Since we have already seen that $\frac{1}{p(\mathcal{D})}$ is independent of $\mu$, we can replace it with a constant $\alpha$.
Since the examples are i.i.d we can derive:
\[
   p(\mu|\mathcal{D}) = \alpha p(\mathcal{D}|\mu) p(\mu) = \alpha \prod_{j=1}^{n} p(\boldsymbol{x}_j|\mu) p(\mu)
\]
From here we can substitute the two Gaussian distributions:
\begin{enumerate}
    \item The distribution of the data given $\mu$;
\[
   p(\boldsymbol{x}_j|\mu) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(\boldsymbol{x}_j - \mu)^2}{2\sigma^2}\right)
\]
\item The prior distribution of $\mu$:
\[
   p(\mu) = \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left(-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}\right)
\]
\end{enumerate}
Substituting these into our equation for the posterior distribution, we get:
\[
   p(\mu|\mathcal{D}) = \alpha \prod_{j=1}^{n} \left( \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(\boldsymbol{x}_j - \mu)^2}{2\sigma^2}\right) \right) \left( \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left(-\frac{(\mu - \mu_0)^2}{2\sigma_0^2}\right) \right)
\]
Which simplifies to:
\[
    p(\mu|\mathcal{D}) = \alpha \prod_{j=1}^{n}  \frac{1}{\sqrt{2\pi}\sigma} \exp\left[-\frac{1}{2}  \frac{(\boldsymbol{x}_j - \mu)^2}{\sigma^2}\right]  \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left[-\frac{1}{2}  \frac{(\mu - \mu_0)^2}{\sigma_0^2}\right]
\]
\[
   = \alpha' \exp \left[ -\frac{1}{2} \left( \sum_{j=1}^{n} \left( \frac{\mu - \boldsymbol{x}_j}{\sigma} \right)^2 + \left( \frac{\mu - \mu_0}{\sigma_0} \right)^2 \right) \right]
\]
\[
   = \alpha'' \exp \left[ -\frac{1}{2} \left[ \left( \frac{n}{\sigma^2} + \frac{1}{\sigma_0^2} \right) \mu^2 - 2 \left( \frac{1}{\sigma^2} \sum_{j=1}^{n} \boldsymbol{x}_j + \frac{\mu_0}{\sigma_0^2} \right) \mu \right] \right]
\]
We can in fact see that this composition still behaves like a normal distribution itself.
\\Solving for $\mu_n$ and $\sigma_n^2$ we have:
\[   \mu_n = \frac{n\sigma_0^2}{n\sigma_0^2 + \sigma^2} \hat{\mu}_n + \frac{\sigma^2}{n\sigma_0^2 + \sigma^2} \mu_0 \]
Where $\hat{\mu}_n$ is the sample mean of the data;
\[   \sigma_n^2 = \frac{\sigma_0^2 \sigma^2}{n\sigma_0^2 + \sigma^2} \]

\paragraph{Interpretation:}
The Interpretation of the results are the following:
\begin{itemize}
    \item the \textbf{mean} of the posterior distribution $\mu_n$ is a \textbf{linear combination} of 
    the prior mean $\mu_0$ and the sample mean $\hat{\mu}_n$;
    \item the \textbf{more training examples} $(n)$ are seen, the more the sample mean 
    $\hat{\mu}_n$ influences the posterior mean $\mu_n$ (the sample mean has $n$ both in the numerator and denominator,
    while the prior mean has only constants in the denominator);
    \item the more training examples are seen, the \textbf{smaller} the variance $\sigma_n^2$ of the posterior 
    distribution becomes, making the distribution sharper around the mean.
\end{itemize}

\paragraph{Computing the conditional density}
Finally, we can compute the conditional density $p(\boldsymbol{x}|\mathcal{D})$ using the posterior distribution:
\[    p(\boldsymbol{x}|\mathcal{D}) = \int p(\boldsymbol{x}|\mu) p(\mu|\mathcal{D}) d\mu \]
\[ \sim \mathcal{N} \left( \mu_n, \sigma^2 + \sigma_n^2 \right) \]
Where:
\begin{itemize}
    \item $\mu_n$ is the posterior mean computed before (the estimation of the mean after seeing the data);
    \item $\sigma^2 + \sigma_n^2$ is the sum of the variance of the original distribution and the variance of the posterior distribution.
\end{itemize} 

\subsection{Sufficient statistics}
Any function on a set of examples $\mathcal{D}$ is called a statistic.
\begin{figure}[H]
    \definition{Sufficient statistic}{
        A statistic $\boldsymbol{s} = \phi(\mathcal{D})$ is said to be sufficient for a parameter $\theta$ if:
        \[ P(\mathcal{D}|\boldsymbol{s}, \theta) = P(\mathcal{D}|\boldsymbol{s}) \]
        If $\theta$ is a random variable, then a \textbf{sufficient statistic} contains all the information 
        that the sample $\mathcal{D}$ provides about the parameter $\theta$.
        \[ p(\theta|\mathcal{D}, s) = p(\theta|\boldsymbol{s}) \] 
        }
\end{figure}
A sufficient statistic allows to summarize a sample $\mathcal{D}$ into a smaller set of values $\boldsymbol{s}$, 
without losing information.
For example \textbf{sample mean} and \textbf{covariance} are sufficient statistics for the parameters of a Gaussian distribution.

\subsection{Conjugate priors}
\begin{figure}[H]
    \definition{Conjugate prior}{Given:
    \begin{itemize}
        \item A likelihood function $p(x|\theta)$;
        \item A prior distribution on the parameter $\theta$, $p(\theta)$;
    \end{itemize}
    The prior distribution $p(\theta)$ is said to be a \textbf{conjugate prior} for the likelihood function $p(x|\theta)$ 
    if the posterior distribution $p(\theta|x)$ is in the same family as the prior distribution $p(\theta)$.}
\end{figure}
This means that if we start with a prior distribution that is conjugate to the likelihood function,
after observing data and updating our beliefs, the posterior distribution will also be in the same family as the prior.

\subsubsection{Example: Bernoulli distribution}
Consider a \textbf{Bernoulli distribution} with events $x = 1$ for success and $x = 0$ for failure. Parameters 
$\theta$ is the probability of success.
The \textbf{probability mass function} is given by:
\[ P(x|\theta) = \theta^x (1 - \theta)^{1-x} \]
The the \textbf{conjugate prior} is a \textbf{Beta distribution} that depends on $\alpha_t$ (number of successes) and $\alpha_h$ (number of failures):
\[ P(\theta|\alpha_t, \alpha_h) = \frac{\Gamma(\alpha)}{\Gamma(\alpha_h) \Gamma(\alpha_t)} \theta^{\alpha_t - 1} (1 - \theta)^{\alpha_h - 1} \]
\paragraph{Maximum likelihood estimation:}
Suppose that a dataset $\mathcal{D} =\{H, H, T, T, T, H,H\}$ of n realizations is given.
\\The likelihood function is given by:
\[ P(\mathcal{D}|\theta) = \theta \cdot \theta \cdot (1-\theta) \cdot (1-\theta) \cdot (1-\theta) \cdot \theta \cdot \theta = \theta^h(1-\theta)^{t})\]
The maximum likelihood estimation of $\theta$ is given by:
\[\frac{\partial}{\partial \theta} \ln p(\mathcal{D}|\theta) = 0 \implies \frac{\partial}{\partial \theta} \left( h \ln \theta + t \ln (1-\theta) \right) = 0\]
\[ \implies \frac{h}{\theta} - \frac{t}{1-\theta} = 0 \implies h (1-\theta) = t \theta \implies \theta = \frac{h}{h + t} \]
$t, h$ are sufficient statistics for the Bernoulli distribution.
\section{Reinforcement Learning}
Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative rewards.
\subsection{Learning Settings}
\begin{itemize}
    \item The learner (agent) is provided with a set of possible states $S$, and for each state, a set of possible actions $A$, moving it to a new state.
    \item In performing action $a$, from state $s$, the agent receives an immediate reward $r$.
    \item The goal is to learn a policy $\pi: S \rightarrow A$ that maximizes the overall reward over time.
    \item The learner has to deal with problems of dealayed reward, where the consequences of an action may not be immediately apparent and trade-offs between exploration (trying new actions) and exploitation (choosing actions known to yield high rewards).
\end{itemize}
More formally, the setting of reinforcement learning can be modeled as a Markov Decision Process (MDP).
\subsection{Markov Decision Process (MDP)}
An MDP is defined by:
\begin{itemize}
    \item A set of \textbf{states} $S$, in which the agent can be;
    \item A set of \textbf{terminal states} $S_{G} \subseteq S$, where the process ends. Might also be empty;
    \item A set of \textbf{actions} $A$ available to the agent;
    \item A \textbf{transition model}, providing the probability of going to state $s'$ when taking action $a$ in state $s$: 
    \[
        P(s'|s,a) ; s,s' \in S, a \in A
    \]
    \item A \textbf{reward function} $R(s,a,s')$, providing the immediate reward received after transitioning from state $s$ to state $s'$ due to action $a$.
\end{itemize}
The agent's objective is to find a policy $\pi(s)$ that maximizes an utility function, taking into account the uncertainty in state transitions and rewards.

\subsection{Utilities}
Utilities are defined over environment histories. An environment history is a sequence of states $[s_0, s_1, s_2, ...]$, passed through by the agent.
When calculating the utility, we assume an infinite horizon, meaning the agent will continue to interact with the environment indefinitely. We also assume stationary preferences, meaning the agent's preferences do not change over time.
\\We can define the utility in two ways:
\begin{itemize}
    \item \textbf{Additive rewards}: values immediate and future rewards equally.
    \[ U([s_0, s_1, s_2, \dots]) = R(s_0) + R(s_1) + R(s_2) + \dots \]
    \item \textbf{Discounted utility}: prefers immediate rewards over future rewards.
    \[ U([s_0, s_1, s_2, \dots]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \dots \]
    where $0 \leq \gamma < 1$ is the discount factor.

\end{itemize}
Here we are assuming that rewards only depend on the current state, not the action taken or the next state.

\subsection{Policy}
A policy $\pi$ is a mapping from states to actions: $\pi: S \rightarrow A$. It defines the action that the agent should take when in a given state.
\paragraph{Expected Utility of a policy}
Given a policy $\pi$, we define the expected utility of $\pi$ as the utility of an environment, taken in expectation over all possible state sequences generated by following policy $\pi$.
In other words, we are multiplying the utility of that history, by the probability of that history occurring under policy $\pi$.
\defib{Optimal Policy}{
An optimal policy $\pi^*$ is a policy that maximizes the expected utility. In other words, it is the best policy the agent can follow to achieve the highest cumulative reward over time.
}
\defib{Utility of states}{
    Given a policy $\pi$, the utility of a state $s$ is defined as:
    \[
        U^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{t = 0}^{\inf} \gamma^t R(S_t)|S_0 = s \right]
    \]
    Where $S_t$ is the state reached after $t$ steps, using policy $\pi$, starting from state $S_0 = s$.
    This is the expected utility of the environment history, starting from state $s$ and following policy $\pi$.
}
We can define the \textbf{true utility} of a state $s$, under the optimal policy $\pi^*$, as:
\[ U^*(s) = U^{\pi^*}(s) \]
Given the true utility of each state, we can easily derive the optimal policy by choosing the action that maximizes the expected utility:
\label{optpolicy}
\[\pi^*(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s,a) U(s') \]
This definition creates a circular dependency between the optimal policy and the true utility, which can be resolved using the Bellman equations.

\subsubsection{Bellman Equations}
The Bellman equation for state $s$ is defined as:
\[ U(s) = R(s) + \gamma \max_{a \in A} \sum_{s' \in S} P(s'|s,a) U(s') \]
This equation states that the utility of a state $s$ is equal to the immediate reward $R(s)$ plus the discounted expected utility of the next state, assuming the agent takes the optimal action.
We can define a system of Bellman equations, one for each state in the MDP. Solving this system will yield the true utilities for all states, which can then be used to derive the optimal policy.
Directly solving this system can be computationally expensive, since they are non-linear equations. Instead, we can use iterative methods such as value iteration or policy iteration to approximate the solution.

\subsubsection{Value Iteration }
Value iteration, also known as Utility Iteration, works by initializing the utility of all states to arbitrary values (for example zero) and then repeatedly updating the utilities using the Bellman equation until convergence.
\begin{algorithm}[H]
    \caption{Value Iteration}
    \begin{algorithmic}[1]
        \State Initialize $U_0(s) \forall s \in S$ to 0
        \Repeat
            \State Update $U_{i+1}(s)$ using the Bellman equation:
            \[
            U_{i+1}(s) \leftarrow R(s) + \gamma \max_{a \in A} \sum_{s' \in S} P(s'|s,a) U_i(s')
            \]
            \State $i \leftarrow i + 1$
        \Until{convergence}
        \State \textbf{return} $U(s)$
    \end{algorithmic}
\end{algorithm}
At each iteration, we update the utility of each state based on the current estimates of the utilities of the successor states. This process continues until the utilities converge to stable values.
The policy can then be derived from the converged utilities using the formula for the optimal policy \ref{optpolicy}.

\subsubsection{Policy Iteration}
Contrary to what happens in value iteration, in policy iteration we start with an arbitrary policy and then iteratively improve it.
\begin{algorithm}[H]
    \caption{Policy Iteration}
    \begin{algorithmic}[1]
        \State Initialize an arbitrary policy $\pi_0$ randomly
        \Repeat
            \State \textbf{Policy Evaluation}: solve the set of linear equations:
            \[
                U_i(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s, \pi_i(s)) U_i(s') \;\; \forall s \in S
            \]
            where $\pi_i(s)$ is the action taken in state $s$ under policy $\pi_i$.
            \State \textbf{Policy Improvement}: update the policy using the new utilities:
            \[
                \pi_{i+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s,a) U_i(s') \;\; \forall s \in S
            \]
        \Until{convergence}
        \State \textbf{return} $\pi(s)$
    \end{algorithmic}
\end{algorithm}
In the policy evaluation step, we compute the utilities of the states under the current policy by solving a system of linear equations. Using the current policy, instead of the max operator from the Bellman equation,
we made the equations linear, making it easier to solve.
In the policy improvement step, we update the policy using equation \ref{optpolicy}, based on the newly computed utilities.

\subsection{Dealing with Partial Knowledge}
Value iteration and policy iteration both assume that the agent has complete knowledge of the MDP.
In most real-world scenarios, the agent does not have access to the full transition model or reward function, but it aims to learn them through interaction with the environment.
\\There are two main approaches to deal with this partial knowledge:
\begin{itemize}
    \item \textbf{Policy Evaluation}: the policy is given, environment is learned through experience;
    \item \textbf{Policy Improvement}: both the policy and the environment are learned through experience.
\end{itemize}
\subsubsection{Adaptive Dynamic Programming (ADP)}
In ADP, the agent maintains counts of visited state and states, given action taken and previous state. These counts are used to do maximum likelihood estimation of the transition model.
\begin{algorithm}
    \caption{Adaptive Dynamic Programming}
    \begin{algorithmic}
        \Repeat
            \State Initialize $s$
            \Repeat
                \State receive reward $r$, set R($s$) = r
                \State choose action $a \leftarrow \pi(s)$
                \State execute action $a$, observe next state $s'$
                \State update counts:
                \[
                    N(s,a) \leftarrow N(s,a) + 1
                \]
                Where $N(s,a)$ is the number of times action $a$ has been taken in state $s$.
                \[
                    N(s'| s,a) \leftarrow N(s'| s,a) + 1
                \]
                Where $N(s'| s,a)$ is the number of times action $a$ has led to state $s'$ from state $s$.
                \State update transition model:
                \[
                    P(s''|s,a) = \frac{N(s''| s,a)}{N(s,a)} \;\; \forall s'', s \in S, a \in A
                \]
                This is the maximum likelihood estimate of the transition probabilities.
                \State update utilities using policy evaluation
                \State $U \leftarrow \text{PolicyEvaluation}(\pi,U, p, R, \gamma)$
            \Until{$s$ is terminal}
        \Until{convergence}
    \end{algorithmic}
\end{algorithm}
The algoritm performs maximum likelihood estimation of the transition model based on observed transitions. It then updates the utilities using standard policy Evaluation.
This approach is really expensive, since it requires computing the policy evaluation at every step.
\subsubsection{Temporal-Difference Learning (TD Learning)}
TD Learning tries to approximate the utility of states, without needing to run full policy evaluation at every step.
\\This is done by assuming a deterministic transition model. If the agent is in state $s$ and takes action $a$, arriving in state $s'$, we assume that $s'$ is the only possible successor of $s$.
\begin{itemize}
    \item If $s'$ was always the successor of $s$, then we can update the utility of $s$ as:
    \[ U(s) = R(s) + \gamma U(s') \]
    \item The temporal-difference update rule updates the utility of $s$ towards the target value:
    \[ U(s) \leftarrow U(s) + \alpha \left( R(s) + \gamma U(s') - U(s)\right) \]
\end{itemize}
Where $\alpha$ is the learning rate, controlling how much new information overrides old information.
\begin{algorithm}
    \caption{TD Learning}
    \begin{algorithmic}
        \Repeat
            \State Initialize $s$
            \Repeat
                \State Receive reward $r$
                \State Choose next action $a \leftarrow \pi(s)$
                \State Execute action $a$, observe next state $s'$
                \State update local utility estimates:
                \[
                    U(s) \leftarrow U(s) + \alpha \left( r + \gamma U(s') - U(s)\right)
                \]
                \State $s \leftarrow s'$
            \Until{$s$ is terminal}
        \Until{convergence}
    \end{algorithmic}
\end{algorithm}
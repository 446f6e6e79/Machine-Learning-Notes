\section{Decision Tree Learning}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{img/decision_tree/GoToLesson.png}
    \caption{An example of a \textbf{learned decision tree}, \textit{Go to lesson}}
    \label{fig:decision_tree}
\end{figure}
A \textbf{decision Tree} represent a \textit{disjunction} ($\lor$: represent the different path to reach the same outcome) of 
\textit{conjunction} ($\land$: represent the \textbf{and} between the conditions on the path to reach the leafs) of constraints over attribute values.
As just described, it's used to encode $DNF$ logical formulas:
\begin{itemize}
    \item Each path from the root to a leaf is a \textbf{conjunction} of the \textbf{constraints} specified on the node along it.
    For example, in \textbf{fig}\ref{fig:decision_tree}, the leftmost path can be read as:
    \[\text{OUTLOOK = RAIN} \land \text{TRANSPORTATION = Uncovered} \]
    \item The leaf contains the \textbf{label} to be assigned to instances reaching it;
    \item The \textbf{disjunction} of all paths is the logical formula represented by the tree.
\end{itemize}

\subsection{Appropriate problems}
There are several tasks where decision trees can be effectively applied. In particular, the task must have these features:
\begin{itemize}
    \item Binary or multi-class \textbf{classification};
    \item Instancies are represented as \textbf{attribute value pair} (tabular data);
    \item Different explanation for the same class are possible (disjunction of different paths);
    \item Some instancies have missing attributes;
    \item There is a \textbf{need} for an \textbf{interpretable explanation} for the output.
    In fact, decision trees can be easily visualized and understood by humans.
\end{itemize}

\subsection{Learning decision tree}
To learn \textbf{decision trees}, there is a simple \textbf{greedy top-down} strategy. 
\\Starting from the \textbf{root node} with the full training set, the algorithm follow these steps:
\begin{enumerate}
    \item Choose best attribute to be evaluated (\textbf{fig}\ref{fig:decision_tree}: \textbf{outlook} is the first chosen attribute);
    \item Add a child node for each attribute value (\textbf{fig}\ref{fig:decision_tree}: rain, overcast and sunny are \textbf{all possible values} for outlook);
    \item Split the node training set into children nodes, according to the attribute values;
    \item \textbf{Stop splitting} a node if all it's training set belongs to a single class (\textbf{homogeneous leaf}), or there are no more attributes to test
\end{enumerate}

\subsubsection{Choosing the best attribute}
To understand how we can choose the best attribute to be evaluated, we first have to define \textbf{entropy}.

\paragraph{Entropy:} a measure of the \textbf{amount of information} contained in a \textbf{collection of instances} $S$ 
which can take a number of $c$ possible values (labels in our case). 
In simpler words, entropy quantifies the amount of uncertainty (or impurity) in a dataset:
\begin{itemize}
    \item If all examples belong to the same class (homogeneous group), then entropy = 0
    \item If examples are evenly split among classes (uncertainty is maximal), then entropy is maximal.
\end{itemize}
It's defined as follows:
\begin{equation}
\label{eq:entropy}
    H(S) = - \sum^c_{i = 1} p_i \log_2 p_i
\end{equation}
Where:
\begin{itemize}
    \item $S$ is the collection of training examples, the dataset or a subset of it;
    \item $p_i$ is the fraction of $S$, with label $i$. 
    It represents the probability of finding the label $i$, inside $S$.
\end{itemize}

\paragraph{Information Gain:} represents the \textbf{expected reduction of entropy}, obtained by partitioning $S$ over an attribute $A$.
It's defined as follows:
\begin{equation}
\label{eq:information-gain}
    IG(S,A) = H(S) - \sum_{v \in Values(A)}\frac{|S_v|}{|S|}H(S_v)
\end{equation}
Where:
\begin{itemize}
    \item $H(S)$: entropy of the original set $S$ (before the split);
    \item $Values(A)$: set of possible values, taken by the attribute $A$;
    \item $S_v$: the subset of $S$, taking value $v$ at attribute $A$;
    \item The second term represents the \textbf{sum of entropies} of all the subsets $S_v$, 
    obtained by partitioning over the attribute $A$, weighted by their respective sizes.
\end{itemize}
We can derive that an attribute with \textbf{high Information Gain} IG tends to produce homogeneous groups in terms of labels, 
thus favoring their classification.

\subsection{Issues in decision tree learning}

\subsubsection{Overfitting avoidance}
Requiring that each leaf contains only examples of a certain class can lead to very complex trees. A complex tree can easily overfit the training set, incorporating noise in the data.
\\To solve this problem, it's possible to accept impure leaves, assigning them the label of the majority of their training examples. This process is called \textbf{pruning}.
There are two possible strategies to prune a decision tree:
\begin{itemize}
    \item \textbf{pre-pruning}: decide whether to stop splitting a node, even if it contains training examples with different labels.
    \item \textbf{post-pruning}: learn the full tree and successively prune it, removing sub-trees.
\end{itemize}

\paragraph{Reduces Error Pruning:} a \textbf{post-pruning} strategy, using the validation set. The procedure follows these next steps:
\begin{enumerate}
    \item for each node in the tree:
    \begin{itemize}
        \item evaluate the performance on the validation set, after removing the subtree rooted at it;
    \end{itemize}
    \item If all node removals made the performance worse, STOP;
    \item Choose the node that gave the best performance improvement;
    \item Replace the subtree rooted at it with a leaf;
    \item Assign to the lead the majority label of all the example in the subtree;
    \item Return to 1.
\end{enumerate}

\subsubsection{Dealing with continuous-valued attributes:}
Continuous valued attributes must be discretized in order to be used as internal node tests, or otherwise, we could end up with an infinite decision tree. The procedure follows these next steps:
\begin{enumerate}
    \item Examples are \textbf{sorted} according to their continuous value;
    \item For \textbf{each pair of successive examples}, having \textbf{different labels}, a \textbf{candidate threshold} is placed at the average of the two values;
    \item For \textbf{each candidate threshold}, the \textbf{IG} obtained by splitting the examples, at the given threshold, is computed;
    \item The threshold with the \textbf{highest IG} is used to discretize the attribute;
\end{enumerate}
\paragraph{Example:} consider the dataset in \textbf{table \ref{tab:continuous-attribute}}:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Example & Attribute A & Class \\
        \hline
        1 & 2.5 & + \\
        2 & 3.0 & + \\
        3 & 4.5 & - \\
        4 & 5.0 & - \\
        5 & 6.0 & + \\
        \hline
    \end{tabular}
    \caption{Example dataset with continuous attribute A}
    \label{tab:continuous-attribute}
\end{table}
The dataset is already sorted according to attribute A. The candidate thresholds are:
\begin{itemize}
    \item Between example 2 and 3: (3.0 + 4.5) / 2 = 3.75;
    \item Between example 4 and 5: (5.0 + 6.0) / 2 = 5.5;
\end{itemize}
We can then compute the IG for both thresholds, choosing the one with the highest value.

\subsubsection{Alternative attributes test measure}
The previously defined \textbf{Information Gain} criterion tends to \textbf{prefer attributes} with a 
\textbf{larger number of possible values}, since it will cause data to be split into very small subsets.
\\As an extreme case, if we consider a unique attribute in our data, such as an ID, it will cause the data to be 
\textbf{perfectly split into singletons}. This may look like a perfect split, but offers no generalization on new examples.

\paragraph{Attribute Value Entropy:} 
To resolve this issue, a measure of the entropy of the dataset $S$, with respect to the values of an attribute $A$ is defined:
\begin{equation}
    H_A(S) = - \sum_{v\in Values(A)} \frac{|S_v|}{S} \log_2 \frac{|S_v|}{S}
\end{equation}
\begin{itemize}
    \item If, for a certain attribute (ex: ID), every value is unique, then $H_A(S)$ is very high;
    \item If a certain attribute has only a few values (ex: binary attribute), then $H_A(S)$ is very low;
\end{itemize}
\paragraph{Information gain Ratio:}
We can then define an \textbf{Information Gain Ratio} measure, that \textbf{down weights} the \textbf{IG} by the \textbf{Attribute Value Entropy}
\begin{equation}
    IGR(S, A) = \frac{IG(S,A)}{H_A(S)}
\end{equation}
Obviously, we have that:
\begin{itemize}
    \item If the attribute $A$ has many unique values, then $H_A(S)$ is high, resulting in a lower \textbf{IGR}, for the attribute $A$;
    \item If the attribute $A$ has few unique values, then $H_A(S)$ is low, resulting in a higher \textbf{IGR}.
\end{itemize}

\subsubsection{Handling Attributes with missing values}
Assuming an \textbf{example} $x$, belonging to \textbf{class} $c(x)$ has a missing value for the attribute $A$.
When the attribute $A$ is to be tested at node $n$, we can apply two solutions:
\begin{itemize}
    \item \textbf{SIMPLE SOLUTION}: Assign, to the example $x$, the most common attribute $A$ values among the training examples at node $n$;
    \item \textbf{COMPLEX SOLUTION}: propagate $x$ to each children of n, with a fractional value equal to the proportion of examples with the corresponding attribute value.
    \\At test times for each candidate class (since more leaf nodes are reached), all fraction belonging to the same class are summed. The example is assigned the class with the highest overall value.
\end{itemize}
\paragraph{Example of complex solution:}
Consider the decision tree in \textbf{fig \ref{fig:decision_tree}} and an example $x$ with missing value for attribute \textbf{outlook}.
\\Suppose that, at node \textbf{outlook}, $x$ is propagated to all three children:
\begin{itemize}
    \item To the left child (\textbf{rain}): with fraction $\frac{5}{10}$ (in the dataset, 5 examples out of 10 have outlook = rain);
    \item To the middle child (\textbf{overcast}): with fraction $\frac{4}{10}$;
    \item To the right child (\textbf{sunny}): with fraction $\frac{1}{10}$;
\end{itemize}
Continuing the propagation, we reach the leafs:
\begin{itemize}
    \item Leftmost leaf (\textbf{yes}): receives fraction $\frac{5}{10}$;
    \item Middle leaf (\textbf{yes}): receives fraction $\frac{4}{10}$;
    \item Rightmost leaf (\textbf{no}): receives fraction $\frac{1}{10}$;
\end{itemize}
Then, summing the fractions for each class:
\begin{itemize}
    \item Class \textbf{yes}: total fraction = $\frac{5}{10} + \frac{4}{10} = \frac{9}{10}$;
    \item Class \textbf{no}: total fraction = $\frac{1}{10}$;
\end{itemize}
The example $x$ is then assigned to class \textbf{yes}, since it has the highest total fraction.

\subsection{Random Forests}
Random Forests are an \textit{ensemble learning method}, that combines multiple decision trees to improve classification or regression performance.
\paragraph{Training}
To train a Random Forest, the following steps are performed:
\begin{enumerate}
    \item Given a training set of $N$ examples, sample $N$ examples with replacement (each example can be selected multiple times);
    \item Train a decision tree on the sampled data, but at each split, choose the best split among a random subset of features, rather than all features.
    This approach allows to introduce more diversity among the trees;
    \item Repeat steps 1 and 2 to create a forest of $M$ trees.
\end{enumerate}
\paragraph{Prediction}
To make predictions with a Random Forest, the following steps are performed:
\begin{itemize}
    \item For classification tasks, each tree in the forest makes a prediction for the input example;
    \item The final prediction is made by majority voting among all trees.
\end{itemize}
A more in depth explanation of ensemble methods can be found in \textbf{section \ref{sec:ensemble-methods}}.
\section{Evaluation}
In this section we will discuss about \textit{Evaluation}, the process of assessing a model's performance, 
reliability, and generalizability using specific quantitative metrics and qualitative techniques.
\begin{itemize}
	\item Requires to define a \textbf{performance measures} to be optimized. 
	These vary based on the task we are addressing;
	\item Performance of learning algorithms cannot be evaluated on the entire domain. We have to
	introduce some approximations;
	\item Performance evaluation is needed for
	\begin{itemize}
		\item \textbf{Tune} the hyper-parameters of learning methods;
		\item \textbf{Evaluating} the performance of the learned model;
		\item \textbf{Computing statistical significance} of results.
	\end{itemize}
\end{itemize}

\subsection{Binary classification}
The typical way to evaluate a binary classifier is by means of a \textbf{confusion matrix}:
\begin{table}[ht]
	\centering
	\begin{tabular}{cc|cc|}
		\cline{3-4}                                         &   & \multicolumn{2}{c|}{prediction} \\
		\cline{3-4}                                         &   & \multicolumn{1}{c|}{P}         & N  \\
		\hline
		\multicolumn{1}{|c|}{ground truth} & P & \multicolumn{1}{c|}{TP}        & FN \\
		\cline{2-4} \multicolumn{1}{|c|}{}                  & N & \multicolumn{1}{c|}{FP}        & TN \\
		\hline
	\end{tabular}
	\caption{Confusion matrix for binary classification}
	\label{fig:binClassConfusionMatrix}
\end{table}
Where:
\begin{itemize}
	\item The rows represent the \textbf{actual} classes (ground truth);
	\item The columns represent the \textbf{predicted} classes;
	\item Each entry in the matrix represents the number of examples that fall into that category:
	\begin{itemize}
		\item \textbf{TP} (True Positive): number of positive examples correctly classified as positive;
		\item \textbf{TN} (True Negative): number of negative examples correctly classified as negative;
		\item \textbf{FN} (False Negative): number of positive examples incorrectly classified as negative;
		\item \textbf{FP} (False Positive): number of negative examples incorrectly classified as positive;
	\end{itemize}
\end{itemize}
From the confusion matrix we can derive several performance measures.

\subsubsection{Accuracy}
\defib{Accuracy}{
	\textit{Accuracy} is the fraction of correctly labelled example among all predictions.
	\begin{equation}
	\mathit{Acc}= \frac{\mathit{TP}+\mathit{TN}}{\mathit{TP}+\mathit{TN}+\mathit{FP}+\mathit{FN}}
	\end{equation}
}
Accuracy metric is not very reliable when the classes are imbalanced. For example, if in a dataset the
positive class represents a small fraction of the examples, a classifier that always predicts negative will achieve 
a high accuracy, but it wouldn't be able to predict any positive example.
\\ A possible solution consists in \textbf{rebalancing costs}, making each positive example worth more than a negative one 
(e.g., each positive counts $\frac{N}{P}$, where $N$ = number of negative examples, and $P$ = number of positive examples).

\subsubsection{Precision and Recall}
\begin{figure}[H]
\defib{Precision}{
	\textit{Precision} is the fraction of correctly predicted positive examples among all predicted positive examples.
	\begin{equation}
	\mathit{Prec}= \frac{\mathit{TP}}{\mathit{TP}+\mathit{FP}}
	\end{equation}
}
\end{figure}
This metric measures the precision of the learner in predicting positive examples. A problem with precision is that
the learner can achieve a high precision by predicting only a few positive examples, only when 100\% sure.

\defib{Recall}{\textit{Recall} (or sensitivity) is the fraction of correctly predicted positive examples among all
actual positive examples.
\begin{equation}
\mathit{Rec}= \frac{\mathit{TP}}{\mathit{TP}+\mathit{FN}}
\end{equation}
}
It measures the ability of the learner to find all positive examples.
\\\\Since these last two metrics are complementary, we can combine them into a single one, called \textbf{F-measure}.
\defib{F-measure}{\textit{F-measure} combines \textbf{precision} and \textbf{recall}, balancing their trade-off.  
\begin{equation}
\mathit{F_\beta} = \frac{(1+\beta^2) \cdot \mathit{Prec} \cdot \mathit{Rec}}{\beta^2 \cdot \mathit{Prec} + \mathit{Rec}}
\end{equation}
}
$F_1$ is usually a good performance measure in the case of unbalanced datasets.

\subsubsection{Precision-Recall curve}
The \textbf{Precision-Recall curve} is a graphical representation that illustrates the trade-off between precision and recall for different threshold values.
\\\\Many classifiers output a confidence (probability score) for each prediction. All the evaluation metrics seen so far
measure the performance of the classifier at a specific threshold.
We can vary this threshold from min to max to obtain different pairs of precision and recall values, which can be plotted to create the Precision-Recall curve.
\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{img/evaluation/precision-recall.png}
	\caption{Precision-Recall curve example}
	\label{fig:precision-recall-curve}
\end{figure}
Also, a single value can be computed by computing the \textbf{area under the curve}. It combines the performance of
the classifier across all thresholds into a single metric.

\subsection{Multi-class classification}
In the case of multi-class classification, we can simply extend the concepts of precision, recall, and F-measure to multiple classes.
We first redefine the \textbf{confusion matrix} as a generalization of the binary case:

\begin{table}[H]
    \centering
    \begin{tabular}{cc|c|c|c|}
        \cline{3-5}                                         &   & \multicolumn{3}{c|}{prediction} \\
        \cline{3-5}                                         &   & \multicolumn{1}{c|}{$y_1$} & \multicolumn{1}{c|}{$y_2$} & \multicolumn{1}{c|}{$y_3$}  \\
        \hline
        \multicolumn{1}{|c|}{ground truth} & $y_1$ & \multicolumn{1}{c|}{$n_{11}$} & \multicolumn{1}{c|}{$n_{12}$} & \multicolumn{1}{c|}{$n_{13}$} \\
        \cline{2-5} \multicolumn{1}{|c|}{} & $y_2$ & \multicolumn{1}{c|}{$n_{21}$} & \multicolumn{1}{c|}{$n_{22}$} & \multicolumn{1}{c|}{$n_{23}$} \\
        \cline{2-5} \multicolumn{1}{|c|}{} & $y_3$ & \multicolumn{1}{c|}{$n_{31}$} & \multicolumn{1}{c|}{$n_{32}$} & \multicolumn{1}{c|}{$n_{33}$} \\
        \hline
    \end{tabular}
    \caption{Confusion matrix for multi-class classification (3 classes example)}
    \label{fig:multiClassConfusionMatrix}
\end{table}
Where:
\begin{itemize}
	\item $n_{ij}$ is the number of examples of class $y_i$ predicted as class $y_j$;
	\item The \textbf{main diagonal} elements $n_{ii}$ represent the number of correctly classified examples for each class, the \textbf{true positives};
	\item The \textbf{sum} of off-diagonal elements in a \textbf{column} $j$ ($\sum_{i \neq j} n_{ij}$) represents the number of \textbf{false positives} for class $y_j$;
	\item The \textbf{sum} of off-diagonal elements in a \textbf{row} $i$ ($\sum_{j \neq i} n_{ij}$) represents the number of \textbf{false negatives} for class $y_i$;
\end{itemize}
From this confusion matrix, we can compute precision and recall for each class $y_i$ as follows:
\begin{itemize}
	\item \textbf{Precision} for class $y_i$:
	\[\mathit{Prec}_i = \frac{n_{ii}}{\sum_{j} n_{ji}}\]
	\item \textbf{Recall} for class $y_i$:
	\[\mathit{Rec}_i = \frac{n_{ii}}{\sum_{j} n_{ij}}\]
\end{itemize}
In addition, we can extend the definition of accuracy to the multiclass setting.
\defib{Multiclass accuracy}{\textit{Multiclass accuracy} is the fraction of correctly predicted examples among all examples.
\begin{equation}
	\mathit{MAcc} = \frac{\sum_{i} n_{ii}}{\sum_{i} \sum_{j} n_{ij}}
\end{equation}}

\subsection{Regression}
The typical performance measures in the regression domain is the \textbf{Root Mean Squared Error} (RMSE).
It represents the \textbf{distance} between the predicted values $f(x_i)$ and the actual values $y_i$.
\defib{Root Mean Squared Error}{\textit{Root Mean Squared Error} is defined as:
\begin{equation}
\mathit{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (f(x_i) - y_i)^2}
\end{equation}
For a dataset $\mathcal{D}$ with $|\mathcal{D}| = N$ examples.
}

\subsection{Performance estimation}
Estimating the performance of a learning algorithm on training data is optimistically biased because, as the model has already seen those examples.
To correctly estimate the performance, we need to evaluate the model on unseen data.
To ensure this, we can use two main strategies.

\subsubsection{Hold-out procedure}
Given a dataset $\mathcal{D}$, we split it into three disjoint subsets:
\begin{itemize}
	\item \textbf{Training set} $\mathcal{D}_{train}$: used to train the model;
	\item \textbf{Validation set} $\mathcal{D}_{val}$: used to tune hyper-parameters and select the best model;
	\item \textbf{Test set} $\mathcal{D}_{test}$: used to evaluate the final performance of the selected model.
\end{itemize} 
This procedure is simple and effective, but can be applied only when we have a sufficiently large dataset are available.

\subsubsection{K-Fold Cross-validation}
The \textbf{k-fold} cross validation approach \textbf{splits} the dataset $\mathcal{D}$ into $k$ disjoint subsets (folds) of equal size.
Then, for each $i \in [1, k]$:
\begin{itemize}
	\item \textbf{Train} a predictor $\mathcal{L}$ using dataset $T_i = \mathcal{D} \setminus \mathcal{D}_i$ (all folds except the $i$-th one);
	\item \textbf{Compute score} $S_i$ of the \textbf{predictor} $\mathcal{L}(T_i)$:
	\[S_i = S_{\mathcal{D}_i}[\mathcal{L}(T_i)]\]
\end{itemize}
After this iterative procedure, we can compute the \textbf{average score} over all folds:
\[\bar{S} = \frac{1}{k} \sum_{i=1}^{k} S_i\]

\begin{comment}
	#TODO: Add variance of k-fold and hypotesis testing
\end{comment}

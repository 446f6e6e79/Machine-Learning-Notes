\section{Evaluation}
In this section we will discuss about \textit{Evaluation}, the process of assessing a model's performance, 
reliability, and generalizability using specific quantitative metrics and qualitative techniques.
\begin{itemize}
	\item Requires to define a \textbf{performance measures} to be optimized. 
	These vary based on the task we are addressing;
	\item Performance of learning algorithms cannot be evaluated on the entire domain. We have to
	introduce some approximations;
	\item Performance evaluation is needed for
	\begin{itemize}
		\item \textbf{Tune} the hyper-parameters of learning methods;
		\item \textbf{Evaluating} the performance of the learned model;
		\item \textbf{Computing statistical significance} of results.
	\end{itemize}
\end{itemize}
The performance measure usually differs from the loss function used during training. In fact:
\begin{itemize}
	\item The \textbf{loss function} simply measures the cost paid for predicting $f(x)$ when the true output is $y$.
	It usually has to be differentiable (smooth) to allow optimization;
	\item The \textbf{performance measure} quantifies how well the model performs on a given task.
	Usually, different performance measures are used to test different aspects of the model.
\end{itemize}

\subsection{Binary classification}
The typical way to evaluate a binary classifier is by means of a \textbf{confusion matrix}:
\begin{table}[ht]
	\centering
	\begin{tabular}{cc|cc|}
		\cline{3-4}                                         &   & \multicolumn{2}{c|}{prediction} \\
		\cline{3-4}                                         &   & \multicolumn{1}{c|}{P}         & N  \\
		\hline
		\multicolumn{1}{|c|}{ground truth} & P & \multicolumn{1}{c|}{TP}        & FN \\
		\cline{2-4} \multicolumn{1}{|c|}{}                  & N & \multicolumn{1}{c|}{FP}        & TN \\
		\hline
	\end{tabular}
	\caption{Confusion matrix for binary classification}
	\label{fig:binClassConfusionMatrix}
\end{table}
Where:
\begin{itemize}
	\item The rows represent the \textbf{actual} classes (ground truth);
	\item The columns represent the \textbf{predicted} classes;
	\item Each entry in the matrix represents the number of examples that fall into that category:
	\begin{itemize}
		\item \textbf{TP} (True Positive): number of positive examples correctly classified as positive;
		\item \textbf{TN} (True Negative): number of negative examples correctly classified as negative;
		\item \textbf{FN} (False Negative): number of positive examples incorrectly classified as negative;
		\item \textbf{FP} (False Positive): number of negative examples incorrectly classified as positive;
	\end{itemize}
\end{itemize}
From the confusion matrix we can derive several performance measures.

\subsubsection{Accuracy}
\definition{Accuracy}{
	\textit{Accuracy} is the fraction of correctly labelled example among all predictions.
	\begin{equation}
	\mathit{Acc}= \frac{\mathit{TP}+\mathit{TN}}{\mathit{TP}+\mathit{TN}+\mathit{FP}+\mathit{FN}}
	\end{equation}
}
Accuracy metric is not very reliable when the classes are imbalanced. For example, if in a dataset the
positive class represents a small fraction of the examples, a classifier that always predicts negative will achieve 
a high accuracy, but it wouldn't be able to predict any positive example.
\\ A possible solution consists in \textbf{rebalancing costs}, making each positive example worth more than a negative one 
(e.g., each positive counts $\frac{N}{P}$, where $N$ = number of negative examples, and $P$ = number of positive examples).

\subsubsection{Precision and Recall}
\begin{figure}[H]
\definition{Precision}{
	\textit{Precision} is the fraction of correctly predicted positive examples among all predicted positive examples.
	\begin{equation}
	\mathit{Prec}= \frac{\mathit{TP}}{\mathit{TP}+\mathit{FP}}
	\end{equation}
}
\end{figure}
This metric measures the precision of the learner in predicting positive examples. A problem with precision is that
the learner can achieve a high precision by predicting only a few positive examples, only when 100\% sure.

\definition{Recall}{\textit{Recall} (or sensitivity) is the fraction of correctly predicted positive examples among all
actual positive examples.
\begin{equation}
\mathit{Rec}= \frac{\mathit{TP}}{\mathit{TP}+\mathit{FN}}
\end{equation}
}
It measures the ability of the learner to find all positive examples.
\\\\Since these last two metrics are complementary, we can combine them into a single one, called \textbf{F-measure}.
\definition{F-measure}{\textit{F-measure} combines \textbf{precision} and \textbf{recall}, balancing their trade-off.  
\begin{equation}
\mathit{F_\beta} = \frac{(1+\beta^2) \cdot \mathit{Prec} \cdot \mathit{Rec}}{\beta^2 \cdot \mathit{Prec} + \mathit{Rec}}
\end{equation}
}
$F_1$ is usually a good performance measure in the case of unbalanced datasets.

\subsubsection{Precision-Recall curve}
The \textbf{Precision-Recall curve} is a graphical representation that illustrates the trade-off between precision and recall for different threshold values.
\\\\Many classifiers output a confidence (probability score) for each prediction. All the evaluation metrics seen so far
measure the performance of the classifier at a specific threshold.
We can vary this threshold from min to max to obtain different pairs of precision and recall values, which can be plotted to create the Precision-Recall curve.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{img/evaluation/precision-recall.png}
	\caption{Precision-Recall curve example}
	\label{fig:precision-recall-curve}
\end{figure}
Also, a single value can be computed by computing the \textbf{area under the curve}. It combines the performance of
the classifier across all thresholds into a single metric.

\subsection{Multi-class classification}
In the case of multi-class classification, we can simply extend the concepts of precision, recall, and F-measure to multiple classes.
We first redefine the \textbf{confusion matrix} as a generalization of the binary case:

\begin{table}[H]
    \centering
    \begin{tabular}{cc|c|c|c|}
        \cline{3-5}                                         &   & \multicolumn{3}{c|}{prediction} \\
        \cline{3-5}                                         &   & \multicolumn{1}{c|}{$y_1$} & \multicolumn{1}{c|}{$y_2$} & \multicolumn{1}{c|}{$y_3$}  \\
        \hline
        \multicolumn{1}{|c|}{ground truth} & $y_1$ & \multicolumn{1}{c|}{$n_{11}$} & \multicolumn{1}{c|}{$n_{12}$} & \multicolumn{1}{c|}{$n_{13}$} \\
        \cline{2-5} \multicolumn{1}{|c|}{} & $y_2$ & \multicolumn{1}{c|}{$n_{21}$} & \multicolumn{1}{c|}{$n_{22}$} & \multicolumn{1}{c|}{$n_{23}$} \\
        \cline{2-5} \multicolumn{1}{|c|}{} & $y_3$ & \multicolumn{1}{c|}{$n_{31}$} & \multicolumn{1}{c|}{$n_{32}$} & \multicolumn{1}{c|}{$n_{33}$} \\
        \hline
    \end{tabular}
    \caption{Confusion matrix for multi-class classification (3 classes example)}
    \label{fig:multiClassConfusionMatrix}
\end{table}
Where:
\begin{itemize}
	\item $n_{ij}$ is the number of examples of class $y_i$ predicted as class $y_j$;
	\item The \textbf{main diagonal} elements $n_{ii}$ represent the number of correctly classified examples for each class, the \textbf{true positives};
	\item The \textbf{sum} of off-diagonal elements in a \textbf{column} $j$ ($\sum_{i \neq j} n_{ij}$) represents the number of \textbf{false positives} for class $y_j$;
	\item The \textbf{sum} of off-diagonal elements in a \textbf{row} $i$ ($\sum_{j \neq i} n_{ij}$) represents the number of \textbf{false negatives} for class $y_i$;
\end{itemize}
From this confusion matrix, we can compute precision and recall for each class $y_i$ as follows:
\begin{itemize}
	\item \textbf{Precision} for class $y_i$:
	\[\mathit{Prec}_i = \frac{n_{ii}}{\sum_{j} n_{ji}}\]
	\item \textbf{Recall} for class $y_i$:
	\[\mathit{Rec}_i = \frac{n_{ii}}{\sum_{j} n_{ij}}\]
\end{itemize}
In addition, we can extend the definition of accuracy to the multiclass setting.
\definition{Multiclass accuracy}{\textit{Multiclass accuracy} is the fraction of correctly predicted examples among all examples.
\begin{equation}
	\mathit{MAcc} = \frac{\sum_{i} n_{ii}}{\sum_{i} \sum_{j} n_{ij}}
\end{equation}}

\subsection{Regression}
The typical performance measures in the regression domain is the \textbf{Root Mean Squared Error} (RMSE).
It represents the \textbf{distance} between the predicted values $f(x_i)$ and the actual values $y_i$.
\definition{Root Mean Squared Error}{\textit{Root Mean Squared Error} is defined as:
\begin{equation}
\mathit{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (f(x_i) - y_i)^2}
\end{equation}
For a dataset $\mathcal{D}$ with $|\mathcal{D}| = N$ examples.
}

\subsection{Performance estimation}
Estimating the performance of a learning algorithm on training data is optimistically biased because, as the model has already seen those examples.
To correctly estimate the performance, we need to evaluate the model on unseen data.
To ensure this, we can use two main strategies.

\subsubsection{Hold-out procedure}
Given a dataset $\mathcal{D}$, we split it into three disjoint subsets:
\begin{itemize}
	\item \textbf{Training set} $\mathcal{D}_{train}$: used to train the model;
	\item \textbf{Validation set} $\mathcal{D}_{val}$: used to tune hyper-parameters and select the best model;
	\item \textbf{Test set} $\mathcal{D}_{test}$: used to evaluate the final performance of the selected model.
\end{itemize} 
This procedure is simple and effective, but can be applied only when we have a sufficiently large dataset are available.

\subsubsection{K-Fold Cross-validation}
The \textbf{k-fold} cross validation approach \textbf{splits} the dataset $\mathcal{D}$ into $k$ disjoint subsets (folds) of equal size.
Then, for each $i \in [1, k]$:
\begin{itemize}
	\item \textbf{Train} a predictor $\mathcal{L}$ using dataset $T_i = \mathcal{D} \setminus \mathcal{D}_i$ (all folds except the $i$-th one);
	\item \textbf{Compute score} $S_i$ of the \textbf{predictor} $\mathcal{L}(T_i)$:
	\[S_i = S_{\mathcal{D}_i}[\mathcal{L}(T_i)]\]
\end{itemize}
After this iterative procedure, we can compute the \textbf{average score} over all folds:
\[\bar{S} = \frac{1}{k} \sum_{i=1}^{k} S_i\]
When applying k-fold cross-validation we obviously get $k$ different performances scores $S_1, S_2, \dots, S_k$.
The average score $\bar{S}$ is a good estimate of the model's performance, but it is also useful to compute the 
\textbf{variance} of these scores to understand the stability of the model across different data splits.
\\\\Assuming that each fold is independent, we can compute the variance as:
\[\mathit{Var}(\bar{S}) = \mathit{Var}[\frac{S_1, S_2, \dots, S_k}{k}]\]

We can take out the constant $\frac{1}{k}$ as $\mathit{Var}(aX) = a^2 \mathit{Var}(X)$:
\[\mathit{Var}(\bar{S}) = \frac{1}{k^2} \sum_{j = 1}^{k} \mathit{Var}(S_j)\]

We cannot compute $\mathit{Var}(S_j)$ directly, but we can estimate it using the sample variance formula:
\[\mathit{Var}(S_j) \approx \frac{1}{k-1} \sum_{i=1}^{k} (S_i - \bar{S})^2\]
Combining the two equations, we get:
\[\mathit{Var}(\bar{S}) \approx \frac{1}{k^2} \sum_{j=1}^{k} \underbrace{\left(\frac{1}{k-1} \sum_{i=1}^{k} (S_i - \bar{S})^2\right)}_{\text{Constant wrt } j}\]
\[ = \frac{1}{k^2} \cdot k \cdot \left(\frac{1}{k-1} \sum_{i=1}^{k} (S_i - \bar{S})^2\right) = \frac{1}{k(k-1)} \sum_{i=1}^{k} (S_i - \bar{S})^2\]

\subsection{Hypothesis testing}
When comparing two learning algorithms, we want to determine if the observed difference 
in performance is statistically significant or if it could have occurred by chance.
Hypothesis testing provides a framework to make this determination.
\\\\Given the hypotheses that we want to test (e.g. "Algorithm $A$ is better than Algorithm $B$"), we first need to define:
\begin{itemize}
	\item \textbf{Null hypothesis} ($H_0$): the one rejecting our claim. It usually states that there is no difference between the two algorithms;
	\item \textbf{Test statistic}: a function $T$ over samples of random variables that summarizes the data. Its value
	is used to decide whether to reject the null hypothesis.
\end{itemize}

\subsubsection{T-test}
In the \textbf{T-test}, the test statistic is given by the standardized mean, defined as:
\begin{equation}
	T = \frac{\bar{X} - \mu_0}{ \sqrt{Var[\bar{X}]}} 
\end{equation}
Assuming that the samples come from an unknown Normal distribution, the test statistic $T$ follows a 
t-distribution with $k-1$ degrees of freedom ($k$ is the number of samples).
\\The null hypothesis $H_0$ is rejected iff the computed statistic $T$ is greater than a critical value $t_{\alpha, k-1}$.
Formally, in a two-tailed test:
\begin{equation}
	H_0 \text{ rejected iff } |T| > t_{\alpha/2, k-1}
\end{equation}
Where $\alpha$ is the significance level (usually set to 0.05, means that we accept a 
5\% chance of rejecting the null hypothesis when it's actually true).

\subsubsection{Example: comparing two algorithms}
Suppose we have two learning algorithms $A$ and $B$, and we want to compare their performance using k-fold cross-validation.
We perform the following steps:
\begin{itemize}
	\item Run \textit{k-fold cross-validation} for both algorithms, obtaining two sets of performance scores;
	\item Compute the \textbf{mean difference} $\hat{\delta}$:
	\begin{equation*}
		\hat{\delta} = \frac{1}{k} \sum_{i=1}^{k} \delta_i
	\end{equation*}
	Where $\delta_i$ is the difference in performance scores for fold $i$. It is computed as:
	\begin{equation*}
		\delta_i = S_{D_i}[L_A(T_i)] - S_{D_i}[L_B(T_i)]
	\end{equation*}
	This is simply the difference between the scores of algorithm $A$ and $B$ on fold $i$.
	\item The null hypothesis $H_0$ states that there is no difference in performance between the two algorithms,
	meaning that $\mu_0 = 0$;
	\item Compute the \textbf{variance} of the differences:
	\begin{equation*}
		Var[\hat{\delta}] = \frac{1}{k(k-1)} \sum_{i=1}^{k} (\delta_i - \hat{\delta})^2
	\end{equation*}
	\item Compute the \textbf{test statistic} $T$:
	\begin{equation*}
		T = \frac{\hat{\delta} - 0}{\sqrt{Var[\hat{\delta}]}} = \frac{\hat{\delta}}{\sqrt{Var[\hat{\delta}]}}
	\end{equation*}
	\item Compare $|T|$ with the critical value $t_{\alpha/2, k-1}$, that can be found in t-distribution tables.
	If $|T| > t_{\alpha/2, k-1}$, we reject the null hypothesis $H_0$ and conclude that there is a statistically significant difference
	between the two algorithms.
\end{itemize}
\section{Kernel Machines}
With what we have seen so far about SVMs, we find out that:
\begin{itemize}
    \item \textbf{Hard Margin SVMs} can address linearly separable data only;
    \item \textbf{Soft Margin SVMs} can address linearly separable problems with some degree of misclassification;
    \item \textbf{Non-linearly separable problems} need higher expressive power to be addressed.
\end{itemize}
\textbf{Kernel Machines} are an extension of SVMs that can address non-linearly separable problems, mantaining the same underlying principles of SVMs (large margin and theoretical guarantees).
\\The key idea is to \textit{map} the input data into a higher dimensional space, where it is more likely to be linearly separable and perform a linear separation in that space.
\subsection{Feature Map}
\defib{Feature Map}{
    \[
        \phi: \mathcal{X} \rightarrow \mathcal{H}
    \]
    A feature map $\phi$ is a function that maps the input space $\mathcal{X}$ to a higher dimensional (possibly infinite-dimensional) feature space $\mathcal{H}$.
    \\\\All the examples $\boldsymbol{x}_i$ are replaced by their images $\phi(\boldsymbol{x}_i)$ in the feature space. 
    This should increase the expressive power of the representation, introducing features that are combinations of the original inputs.
}
An example of feature map is the following:
\defib{Polynomial Feature Map}{
    \begin{itemize}
        \item \textbf{Homogeneous Polynomial Feature Map} of degree $2$:
    \begin{equation} \label{eq:homogeneous_polynomial_feature_map}
        \phi\!\left(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\right) =
        \begin{pmatrix}
            x_1^2 \\
            x_1 x_2 \\
            x_2^2 \\
        \end{pmatrix}
    \end{equation}
    This feature map takes a 2D input vector $\boldsymbol{x} = (x_1, x_2)$ and maps it to a 3D feature space by including polynomial terms of degree 2.
    \item \textbf{Non-Homogeneous Polynomial Feature Map} of degree $2$:
    \begin{equation} \label{eq:non_homogeneous_polynomial_feature_map}
        \phi\!\left(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\right) =
        \begin{pmatrix}
            x_1^2 \\
            x_1 x_2 \\
            x_2^2 \\
            1
        \end{pmatrix}
    \end{equation}
    \end{itemize}
    This feature map is similar to the previous one but includes a bias term (constant $1$) to account for non-homogeneous polynomials.
}
\subsection{Linear separation in feature space}
Once the data is mapped into the feature space using the feature map $\phi$, we can apply SVM algorithms, replacing $\boldsymbol{x}$ with $\phi(\boldsymbol{x})$:
\begin{equation}
    f(x) = \boldsymbol{w}^T \phi(\boldsymbol{x}) + w_0
\end{equation}
A linear separation in the feature space corresponds to a non-linear separation in the original input space.
\\E.g. using the homogeneous polynomial feature map of degree $2$, the decision function becomes:
\[
    \phi\!\left(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\right) = sgn(w_1 x_1^2 + w_2 x_1 x_2 + w_3 x_2^2 + w_0)
\]
The linear separator in the $3D$ feature space corresponds to a ellipse in the original $2D$ input space.
\subsection{Kernel trick}
Computing the feature map $\phi(\boldsymbol{x})$ explicitly can be computationally expensive. In the \textbf{dual formulation} 
of SVMs, the data points appear only in the form of \textbf{dot products} $\langle \phi(\boldsymbol{x}), \phi(\boldsymbol{x}') \rangle$.
\defib{Kernel trick}{
The \textbf{kernel trick} consists in replacing the dot product in the feature space with an equivalent \textbf{kernel function} 
    \begin{equation}
        \langle \phi ( \boldsymbol{x}), \phi (\boldsymbol{x}') \rangle
        = \phi(\boldsymbol{x})^T \phi(\boldsymbol{x}') 
        = K(\boldsymbol{x}, \boldsymbol{x}')
    \end{equation}
    The \textbf{kernel functions} uses examples in the \textbf{original input space} $\mathcal{X}$ to compute the dot product in the \textbf{feature space} $\mathcal{H}$, without explicitly computing the feature map $\phi$.
}
\subsubsection{Examples of kernels}
\paragraph{Homogeneous Polynomial Kernel}:
taking in consideration the equation \ref{eq:homogeneous_polynomial_feature_map}, we can derive the following kernel:
\begin{equation}
    K(\boldsymbol{x}, \boldsymbol{x}') = (\boldsymbol{x}^T \boldsymbol{x}')^d
\end{equation}
Setting $d=2$, we have:
\[
    K(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}, \begin{pmatrix} x_1' \\ x_2' \end{pmatrix}) = (x_1 x_1' + x_2 x_2')^2
\]
\[
    = (x_1 x_1')^2 + 2 x_1 x_2 x_1' x_2' + (x_2 x_2')^2
\]
\[
= \begin{pmatrix}
    x_1^2 \\
    \sqrt{2} x_1 x_2 \\
    x_2^2
\end{pmatrix}^T
\begin{pmatrix}
    x_1'^2 \\
    \sqrt{2} x_1' x_2' \\
    x_2'^2
\end{pmatrix}
\]
This is equivalent to the dot product in the feature space defined by the homogeneous polynomial feature map of degree $2$, up to a scaling factor $\sqrt{2}$.
\paragraph{Non-Homogeneous Polynomial Kernel}:
taking in consideration the equation \ref{eq:non_homogeneous_polynomial_feature_map}, we can derive the following kernel:
\begin{equation}
    K(\boldsymbol{x}, \boldsymbol{x}') = (\boldsymbol{x}^T \boldsymbol{x}' + 1)^d
\end{equation}
Setting $d=2$, we have:
\[
    K(\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}, \begin{pmatrix} x_1' \\ x_2' \end{pmatrix}) = (x_1 x_1' + x_2 x_2' + 1)^2
\]
\[
    = (x_1 x_1')^2 + 2 x_1 x_2 x_1' x_2' + (x_2 x_2')^2 + 2 x_1 x_1' + 2 x_2 x_2D   + 1
\]
\[= \begin{pmatrix}
    x_1^2 \\
    \sqrt{2} x_1 x_2 \\
    x_2^2 \\
    \sqrt{2} x_1 \\
    \sqrt{2} x_2 \\
    1
\end{pmatrix}^T
\begin{pmatrix}
    x_1'^2 \\
    \sqrt{2} x_1' x_2' \\
    x_2'^2 \\
    \sqrt{2} x_1' \\
    \sqrt{2} x_2' \\
    1
\end{pmatrix}
\]
This is equivalent to the dot product in the feature space defined by the non-homogeneous polynomial feature map of degree $2$, up to a scaling factor $\sqrt{2}$.
\subsubsection{Valid Kernels}
\defib{Valid Kernel}{
    A kernel function $K : \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is valid if it corresponds to a dot product in some feature space $\mathcal{H}$:
    \begin{equation}
        K(\boldsymbol{x}, \boldsymbol{x}') = \langle \phi(\boldsymbol{x}), \phi(\boldsymbol{x}') \rangle
    \end{equation}
}
The kernel generalizes the concept of similarity between data points in arbitrary feature spaces.
\defib{Gram Matrix}{
    Given examples $\{\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_m\}$, and a kernel function $k$, the \textbf{Gram matrix} $K$ is defined as:
    \begin{equation}
        K_{ij} = k(\boldsymbol{x}_i, \boldsymbol{x}_j) \forall i, j
    \end{equation}
}
\defib{Positive definite matrix}{
    A matrix $M \in \mathbb{R}^{m \times m}$ is positive definite if:
    \begin{equation}
        \sum_{i,j = 1}^m c_i c_j K_{ij} \geq 0 \quad \forall \boldsymbol{c} \in \mathbb{R}^m
    \end{equation}
}
A sufficient and necessary condition for a kernel $K$ to be valid is that the corresponding Gram matrix is positive definite for any choice of examples $\{\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_m\}$.
There are several ways to check if a kernel is valid, e.g.:
\begin{itemize}
    \item Prove its positive definiteness;
    \item Find out a corresponding feature map $\phi$.
    \item Use kernel combination properties (e.g. sum, product yield valid kernels).
\end{itemize}


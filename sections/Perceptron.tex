\section{Perceptron}
Perceptron are one of the simplest types of artificial neural networks, based on the concept of linear classifiers (\ref{sec:linear_discriminant_functions}).
They are based on the idea of simulating the behavior of a biological neuron.
\subsection{Biological Motivation}
The human brain is composed of densily interconnected neurons. They process information based on the following principles:
\begin{itemize}
    \item Electrochemical reactions allow signals to propagate through the neuron;
    \item Sinapses connect neurons together, they can either \textbf{excite} or \textbf{inhibit} the receiving neuron;
    \item When the incoming signals exceed a certain threshold, the neuron \textbf{fires} and transmits the signal to the next neuron.
\end{itemize}
Artificial neural networks aim to mimic this behavior using mathematical models. 
The threshold is modeled using an \textbf{activation function}, while the synapses are represented by \textbf{weights}.

\subsection{Single Neuron Architecture}
A single neuron perceptron is a specific learning model that implements a linear binary classifier. It can be represented as:
\begin{equation}   
    \label{eq:perceptron}
    f(\boldsymbol{x}) = sign(\boldsymbol{w}^T \boldsymbol{x} + w_0)
\end{equation}
The function is a \textbf{linear combination} of the input features $\boldsymbol{x}$.
\begin{figure}[H]
    \begin{tikzpicture}
        \node[input-node] (bias) at (0, 2) {1};
        \node[left=0.2cm of bias] {\textbf{Bias}};
        
        \node[input-node] (x1) at (0, 1) {$x^1$};
        \node[input-node] (x2) at (0, 0) {$x^2$};
        \node (dots) at (0, -0.8) {$\vdots$};
        \node[input-node] (xm) at (0, -1.8) {$x^m$};
        
        \node[left=0.5cm of x2] (inputs-label) {Inputs};

        % 2. Draw Summation and Activation Nodes
        \node[sum-node] (sum) at (4, 0.4) {\Large $\Sigma$};
        \node[activation-node, right=1.2cm of sum] (act) {
            \begin{tikzpicture}[scale=0.2] 
                \draw[blue, thick] (-1,-1) -- (0,-1) -- (0,1) -- (1,1); 
            \end{tikzpicture}
        };
        % Connection
        \draw[connection] (bias) -- (sum) node[midway, above] {$w_0$};
        \draw[connection] (x1) -- (sum) node[midway, above] {$w_1$};
        \draw[connection] (x2) -- (sum) node[midway, above] {$w_2$};
        \draw[connection] (xm) -- (sum) node[midway, above] {$w_m$};
        \draw[connection] (sum) -- (act);
        \draw[connection] (act) -- ++(2,0) node[right] {Output};

        \node[above=0.3cm of act] {Activation func};

    \end{tikzpicture}
\end{figure}
\subsubsection{Augmented feature / weight vector}
To simplify the notation, we can introduce an \textbf{augmented feature vector}. We can rewrite the 
equation \eqref{eq:perceptron} as:
\begin{equation}
    f(\boldsymbol{x}) = sign(\hat{\boldsymbol{w}}^T \hat{\boldsymbol{x}})
\end{equation}
where the \textbf{bias term} is included in the weight vector and the feature vector as follows:
\[
    \hat{\boldsymbol{w}} = 
    \begin{bmatrix}
    w_0 \\
    \boldsymbol{W}
    \end{bmatrix}
    , \quad
    \hat{\boldsymbol{x}} = 
    \begin{bmatrix}
    1 \\
    \boldsymbol{x}
    \end{bmatrix}
\]
In the following sections, we will skip the hat notation for simplicity.

\subsubsection{Representational power}
The single neuron perceptron can only represent \textbf{linearly separable} functions. 
Examples of this are the \textbf{primitive boolean functions} such as \textbf{AND}, \textbf{OR}, and \textbf{NOT}.
This implies that any logic formula can be represented by a network of a 2 level perceptron in \textbf{disjunctive normal form} (DNF) or \textbf{conjunctive normal form} (CNF).

\subsection{Parameter learning}
Similar to what we saw for \textbf{maximum likelihood estimation}, we need to find a \textbf{function} of the parameters
to be optimized.
\\\\In this case, a reasonable choice is to minimize the \textbf{measure of error} on the training set $\mathcal{D}$. 
This is called \textbf{loss function} $\ell$ and it compares the predicted output $f(\boldsymbol{x}_i)$ 
with the ground truth $y_i$.
\\We can define \textbf{training error} $E$ as:
\begin{equation}
    E(\boldsymbol{w}, D) = \sum_{(\boldsymbol{x},y) \in \mathcal{D}} \ell(f(\boldsymbol{x}), y)
\end{equation}
This is simply the sum of the losses over all training examples.

\subsubsection{Gradient descent}
This is an optimization problem where we want to find the weights $\boldsymbol{w}$ that minimize the error function $E$.
There is no closed-form solution for this problem, so we need to use an iterative optimization algorithm.
\\The common approach is to minimize the error function using a \textbf{gradient descent} approach:
\begin{enumerate}
    \item Initialize the weights $\boldsymbol{w}$ randomly or with zeros.
    \item Iterate until gradient is approximately zero, updating the weights as:
    \begin{equation}
        \boldsymbol{w} = \boldsymbol{w}' - \eta \nabla E(\boldsymbol{w}', \mathcal{D})
    \end{equation}
    where $\eta$ is the learning rate and controls the amount of movement at each step.
\end{enumerate}
Since the gradient $\nabla E(\boldsymbol{w}; D)$ points in the direction of \textbf{maximum increase} of the error function,
we multiply it by $-1$ to move in the direction of \textbf{maximum decrease}.
This approach is guaranteed to converge to a local optimum of the error function $E(\boldsymbol{w}; D)$ for small enough learning rates $\eta$.
\begin{itemize}
    \item too \textbf{low} learning rate: slow convergence;
    \item too \textbf{high} learning rate: the algorithm may \textbf{diverge} (oscillate around the minimum).
\end{itemize}

\subsubsection*{Perceptron training rule}
In order to apply a \textbf{gradient descent} approach, we need to define a \textbf{loss function} which is \textbf{differentiable} (smooth).
As a consequence, we adopt a different training rule, called \textbf{perceptron training rule}. It's defined as follows:
\begin{equation}\label{eq:perceptron_error}
    E(\boldsymbol{w}; \mathcal{D}) = \sum_{(\boldsymbol{x}, y) \in \mathcal{D}_E} -yf(\boldsymbol{x})
\end{equation}
where $\mathcal{D}_E$ is the set of \textbf{misclassified} samples in the training set $\mathcal{D}$, for which:
\[ yf(\boldsymbol{x}) \leq 0 \]
The error is the \textbf{sum} of the \textbf{functional margins} (confidences) of the misclassified samples.
\\Applying gradient descent to the error function in \eqref{eq:perceptron_error}, we obtain:
\[\nabla E(\boldsymbol{w}; \mathcal{D}) = \nabla \sum_{(\boldsymbol{x}, y) \in \mathcal{D}_E} -yf(\boldsymbol{x})\]
\[=\nabla \sum_{(\boldsymbol{x}, y) \in \mathcal{D}_E} -y(\boldsymbol{w}^T \boldsymbol{x}) \]
\[= \sum_{(\boldsymbol{x}, y) \in \mathcal{D}_E} -y \boldsymbol{x} \]
Thus, the amount of change in the weights is:
\[- \eta \nabla E(\boldsymbol{w}; \mathcal{D}) = \eta \sum_{(\boldsymbol{x}, y) \in \mathcal{D}_E} y \boldsymbol{x} \]
\subsubsection{Stochastic Gradient Descent}
In practice, the \textbf{batch gradient descent} approach we discussed so far is rarely used, 
since it requires to compute the gradient over the entire training set $\mathcal{D}$ at each iteration.
\\A more common approach is to use \textbf{stochastic gradient descent} (SGD). It works as follows:
\begin{enumerate}
    \item Initialize the weights $\boldsymbol{w}$ randomly;
    \item Iterate until all examples are correctly classified:
    \begin{enumerate}
        \item For each incorrectly classified example $(\boldsymbol{x}, y)$ in the training set $\mathcal{D}$, update the weights as:
        \begin{equation}
            \boldsymbol{w} = \boldsymbol{w} + \eta y \boldsymbol{x}
        \end{equation}
    \end{enumerate}
\end{enumerate} 
With this approach, the weights are updated after each misclassified example, 
rather than after processing the entire training set.
Each gradient step is \textbf{very fast} and can sometimes avoid local minima.

\subsection{Perceptron Regression}
Linear models can also be used for \textbf{regression} tasks, where the goal is to predict a continuous output variable $y$ given an input feature vector $\boldsymbol{x}$.
The problem can be modeled as:
\begin{itemize}
    \item Let $X \in \mathbb{R}^{n \times d}$ be the input training matrix (i.e. $X = \left[ \boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n \right]^T$), where each row is a $d$-dimensional feature vector and $n = |\mathcal{D}|$;
    \item Let $\boldsymbol{y} \in \mathbb{R}^n$ be the output training Matrix (i.e. $\boldsymbol{y}_i$ is the target value for the i-th training example);
    \item \textbf{Regression learning} can be stated as a set of \textbf{linear equations}:
    \begin{equation*}
        X\boldsymbol{w} = \boldsymbol{y}
    \end{equation*}
    Giving as a solution:
    \begin{equation}
        \boldsymbol{w} = X^{-1}\boldsymbol{y}
    \end{equation}
\end{itemize}
However this approach has a few problems:
\begin{itemize}
    \item The matrix $X$ is usually not square, so the inverse $X^{-1}$ is not defined;
    \item System of equations is overdetermined (more equations than unknowns) $\implies$ no exact solution exists.
\end{itemize}

\subsubsection{Error function}
We can again resort to loss minimization to find an approximate solution.
Our goal is not to solve the system exactly, but to find the weights $\boldsymbol{w}$ that minimize the \textbf{mean squared error} (MSE) on the training set:
\begin{equation}*
    E(\boldsymbol{w}; \mathcal{D}) = \sum_{(\boldsymbol{X}, y) \in \mathcal{D}} (y - f(\boldsymbol{x}))^2
\end{equation}
This can be rewritten in matrix form as:
\begin{equation}
    \label{eq:mse}
    E(\boldsymbol{w}; \mathcal{D}) = (\boldsymbol{y} - X\boldsymbol{w})^T (\boldsymbol{y} - X\boldsymbol{w})
\end{equation}
This is a \textbf{convex} function, so it has a single global minimum with a closed-form solution. 
\textbf{Gradient descent} can still be faster than computing the closed-form solution, especially for large datasets.

\subsubsection{Closed form solution}
To find the weights $\boldsymbol{w}$ that minimize the mean squared error, we can set the gradient 
of the error function (\ref{eq:mse}) to zero and solve for $\boldsymbol{w}$:
\[\nabla E(\boldsymbol{w}; \mathcal{D}) = \nabla (\boldsymbol{y} - X\boldsymbol{w})^T (\boldsymbol{y} - X\boldsymbol{w})\]
\[ 2(\boldsymbol{y} - X\boldsymbol{w})^T (-X) = 0\]
\[-2\boldsymbol{y}^TX + 2\boldsymbol{w}^T X^T X = 0\]
Rearranging the terms and dividing by 2, we get:
\[\boldsymbol{w}^T X^T X = \boldsymbol{y}^T X\]
Taking the transpose of both sides:
\[X^T X \boldsymbol{w} = X^T \boldsymbol{y}\]
Assuming that $X^T X$ is invertible, we can solve for $\boldsymbol{w}$:
\begin{equation}
    \boldsymbol{w} = (X^T X)^{-1} X^T \boldsymbol{y}
\end{equation}
NOTE:
\begin{itemize}
    \item $(X^T X)^{-1} X^T$ is a pseudoinverse called the \textbf{left inverse} of $X$;
    \item If $X$ is square, than the left inverse is equal to the regular inverse $X^{-1}$, and this correspomonds to the exact solution of the system of equations;
    \item The left inverse exists only if the columns of $X$ are linearly independent. If this is not the case, we can use \textbf{regularization} techniques to make the matrix invertible.
\end{itemize}

\subsubsection{Gradient descent}
In case of large datasets, computing the closed-form solution can be computationally expensive. 
In such cases, we can use \textbf{gradient descent} to iteratively update the weights until convergence.
\[\frac{\partial E}{\partial \boldsymbol{w}_i} = \frac{\partial}{\partial \boldsymbol{w}_i} \frac{1}{2} \sum_{(\boldsymbol{X}, y) \in \mathcal{D}} (y - f(\boldsymbol{x}))^2\]
\[=\frac{1}{2} \sum_{(\boldsymbol{X}, y) \in \mathcal{D}} \frac{\partial}{\partial \boldsymbol{w}_i}(y-f(\boldsymbol{x}))^2\]
\[=\frac{1}{2} \sum_{(\boldsymbol{X}, y) \in \mathcal{D}} 2(y-f(\boldsymbol{x}))\frac{\partial}{\partial \boldsymbol{w}_i} (y - \boldsymbol{w}^T \boldsymbol{x})\]
\[= \sum_{(\boldsymbol{X}, y) \in \mathcal{D}} (y - f(\boldsymbol{x})) (-x_i)\]
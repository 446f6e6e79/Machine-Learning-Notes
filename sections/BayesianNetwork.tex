\section{Bayesian Networks}
All probabilistic inference and learning methods are based on the repeated application of basic rules of probability theory.
However, as the number of random variables increases, the complexity of representing and manipulating probability distributions grows exponentially.
Probabilistic graphical models provide a convenient framework to represent and manipulate complex probability distributions.
\\\\Bayesian Networks (BNs) are a formalism to represent probabilistic relationships among a set of random variables.
They allow to:
\begin{itemize}
    \item Visualize the structure of a probabilistic model;
    \item Discover properties of the model (e.g., conditional independencies);
    \item Efficiently perform probabilistic inference and learning, using the structure of the graph;
    \item Represent multiple distributions with the same graph, independently of their qualitative aspects
    (e.g., discrete vs continuous variables).
\end{itemize}

\subsection{Bayesian Networks structure}
A Bayesian Network (BN) structure $\mathcal{G}$ is a directed acyclic graph (DAG) where:
\begin{itemize}
    \item Each node represents a random variable;
    \item Each directed edge represent a direct dependency between the two connected variables;
\end{itemize}
It's important to note that the absence of an edge between two nodes doesn't imply independence.
\\\\The given structure $\mathcal{G}$ of a BN encodes these independence assumptions:
\begin{equation}
    \label{eq:localIndependencies}
    \mathcal{I}_l(\mathcal{G}) = \{ \forall i\; \boldsymbol{x}_i \perp \text{NonDescendants}_{\boldsymbol{x}_i} | \text{Parents}_{\boldsymbol{x}_i} \}
\end{equation}
Each variable $\boldsymbol{x}_i$ is independent of its non-descendants given its parents. 

\begin{figure}[H]
\centering
    \begin{tikzpicture}
    % Nodes
    \node[bn-node] (x1) at (0, 3.5) {$x_1$};
    \node[bn-node] (x2) at (-1.4, 2.8) {$x_2$};
    \node[bn-node] (x3) at (1.4, 2.8) {$x_3$};
    \node[bn-node] (x4) at (-0.8, 1.5) {$x_4$};
    \node[bn-node] (x5) at (0.8, 1.5) {$x_5$};
    \node[bn-node] (x6) at (-1.2, 0) {$x_6$};
    \node[bn-node] (x7) at (0.8, 0) {$x_7$};
    % Edges
    \draw[bn-arrow] (x1) -- (x4);
    \draw[bn-arrow] (x1) -- (x5);
    \draw[bn-arrow] (x2) -- (x4);
    \draw[bn-arrow] (x3) -- (x4);
    \draw[bn-arrow] (x3) -- (x5);
    \draw[bn-arrow] (x4) -- (x6);
    \draw[bn-arrow] (x4) -- (x7);
    \draw[bn-arrow] (x5) -- (x7);
    \end{tikzpicture}
    \caption{Example of Bayesian Network}
\end{figure}
\subsubsection{Independency Map}
\definition{Independency Map}{
    \begin{itemize}
        \item Let $p$ be a distribution over a set of random variables $\boldsymbol{X}$;
        \item Let $\mathcal{I}(p)$ be the set of independencies that hold in $p$;
    \end{itemize}
    We say that $\mathcal{G}$ is an \textbf{Independency Map} (I-map) for $p$ if $p$ satisfies
    the local independencies encoded in $\mathcal{G}$:
    \[ \mathcal{I}_l(\mathcal{G}) \subseteq \mathcal{I}(p) \]
}
This means that all independencies encoded in the graph $\mathcal{G}$ hold in the distribution $p$.
There can be additional independencies in $p$ that are not encoded in $\mathcal{G}$.

\subsubsection{Factorization}
We say that a distribution $p$ \textbf{factorizes} according to a BN structure $\mathcal{G}$ if $p$ can be expressed as:
\[ p(\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_m) = \prod_{i=1}^{m} p(\boldsymbol{x}_i | \text{Parents}_{\boldsymbol{x}_i}) \]
We have that $\mathcal{G}$ is an I-map for $p$ if and only if $p$ factorizes according to $\mathcal{G}$.
\begin{proof}
(\textit{I-Map}$\Rightarrow$\textit{Factorization}) 
\begin{itemize}
    \item Assume that $\mathcal{G}$ is an I-map for $p$;
    \item We know that $p$ satisfies the local independencies encoded in $\mathcal{G}$:
    \[ \{\forall i\; \boldsymbol{x}_i \perp \text{NonDescendants}_{\boldsymbol{x}_i} | \text{Parents}_{\boldsymbol{x}_i} \}\]
    \item We can order the variables in a topological order, such that each variable always comes after its parents;
    \item We can decompose the joint distribution using the chain rule of probability:
    \[ p(x_1, x_2, \ldots, x_m) = \prod_{i=1}^{m} p(x_i | x_1, x_2, \ldots, x_{i-1}) \]
    \item Since we ordered the variables in a topological order, now $\text{Parents}_{x_i} \subseteq \{x_1, x_2, \ldots, x_{i-1}\}$.
    We derive that:
    \[ p(x_i | x_1, x_2, \ldots, x_{i-1}) = p(x_i | \text{Parents}_{x_i}) \]
\end{itemize}
Combining the last two equations, we obtain the factorization property:
\[ p(x_1, x_2, \ldots, x_m) = \prod_{i=1}^{m} p(x_i | \text{Parents}_{x_i}) \]

(\textit{Factorization}$\Rightarrow$\textit{I-Map}) 
\begin{itemize}
    \item If $p$ factorizes according to $\mathcal{G}$, then the joint distribution can be expressed as:
    \[ p(x_1, x_2, \ldots, x_m) = \prod_{i=1}^{m} p(x_i | \text{Parents}_{x_i}) \]
    \item Consider now the variable $x_m$; By the product rule ($p(X, Y) = p(Y|X) p(X)$), we can express the conditional distribution as:
    \[ p(x_m | x_1, x_2, \ldots, x_{m-1}) = \frac{p(x_1,\ldots, x_m)}{p(x_1, \ldots, x_{m-1})} \]
    \item We can apply the sum rule to the denominator, marginalizing over $x_m$:
    \[ p(x_m | x_1, \ldots, x_{m-1}) = \frac{p(x_1,\ldots, x_m)}{\sum_{x_m} p(x_1,\ldots, x_m)} \]
    \item Applying the factorization property, we obtain:
    \[ = \frac{\prod_{i=1}^{m} p(x_i | \text{Parents}_{x_i})}{\sum_{x_m} \prod_{i=1}^{m} p(x_i | \text{Parents}_{x_i})} \]
    \item We can rearrange the equation isolating the term for $x_m$:
    \[ = \frac{p(x_m| \text{Parents}_{x_m}) \prod_{i=1}^{m-1} p(x_i | \text{Parents}_{x_i})}{\sum_{x_m} p(x_m| \text{Parents}_{x_m}) \prod_{i=1}^{m-1} p(x_i | \text{Parents}_{x_i})} \]
\end{itemize}
At this point we can simplify the equation, since $\sum_{x_m} p(x_m| \text{Parents}_{x_m}) = 1$, obtaining:
\[
p(x_m | x_1, \ldots, x_{m-1}) = p(x_m| \text{Parents}_{x_m})
\]
\end{proof}

\subsection{Bayesian Network definition}
A Bayesian Network (BN) is a pair $\mathcal{B} = (\mathcal{G}, \mathcal{P})$, where:
\begin{itemize}
    \item $\mathcal{G}$ is a directed acyclic graph (DAG) representing the structure of the BN;
    \item $\mathcal{P}$ is a set of conditional probability distributions (CPDs) associated with the nodes in the graph.
\end{itemize}
$P$ must factorize according to $\mathcal{G}$:
\[ P(\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_m) = \prod_{i=1}^{m} P(\boldsymbol{x}_i | \text{Parents}_{\boldsymbol{x}_i}) \]

\paragraph{Example:}
Consider the following BN structure:
\begin{figure}[H]
\centering
    \begin{tikzpicture}
    % Nodes
    \node[bn-node] (A) at (-2, 1) {A};
    \node[bn-node] (B) at (2, 1) {B};
    \node[bn-node] (C) at (0, -1) {C};
    % Edges
    \draw[bn-arrow] (A) -- (C);
    \draw[bn-arrow] (B) -- (C);
    \end{tikzpicture}
    \caption{Example of Bayesian Network structure}
\end{figure}
\begin{itemize}
    \item Gene $A$ and Gene $B$ are independent;
    \item Gene $C$ can be influenced by both Gene $A$ and Gene $B$;
\end{itemize}
\begin{center}
    \begin{minipage}{0.4\textwidth}
        \centering
        \label{tab:cpdGeneA}
        \begin{tabular}{c|c|c}
            Gene &  Value & P(value)\\ 
            \hline
            A & active   & 0.3\\
            A & inactive & 0.7\\
        \end{tabular}
        \captionof{table}{CPD for Gene A}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
    \centering
        \label{tab:cpdGeneB}
        \begin{tabular}{c|c|c}
            Gene &  Value & P(value)\\ 
            \hline
            B & active   & 0.3\\
            B & inactive & 0.7\\
        \end{tabular}
        \captionof{table}{CPD for Gene B}
    \end{minipage}
\end{center}

\begin{center}
\begin{tabular}{|c|c|cc|cc|}
\hline
 & & \multicolumn{4}{c|}{A} \\
 & & \multicolumn{2}{c|}{active} & \multicolumn{2}{c|}{inactive} \\
\cline{3-6}
 & & \multicolumn{2}{c|}{B} & \multicolumn{2}{c|}{B} \\
 & & active & inactive & active & inactive \\
\hline
\multirow{2}{*}{C}
 & active   & 0.9 & 0.6 & 0.7 & 0.1 \\
 & inactive & 0.1 & 0.4 & 0.3 & 0.9 \\
\hline
\end{tabular}
\captionof{table}{Conditional Probability Table for Gene C}
\end{center}

\subsection{D-separation}
Before introducing the concept of D-separation, we need to better understand the concept of independence and conditional independence.
We will then present the basic structures that can appear in a BN.

\subsubsection{Independence and conditional independence}
\definition{Independence}{
We usually say that, two variables $a, b$ are independent (written $a \perp b|\empty$) if:
    \[ P(a, b) = P(a) P(b) \]
}
\begin{figure}[H]
\definition{Conditional independence}{
    We say that two variables $a, b$ are conditionally independent given a third variable $c$ (written $a \perp b | c$) if:
    \[ P(a, b | c) = P(a | c) P(b | c) \]
}
\end{figure}
Both independence and conditional independence can be verified by applying basic rules of probability.
Instead, BN allow to verify this properties through the concept of D-separation.

\subsubsection{Tail-to-tail connection}
In a tail-to-tail connection, two variables $a$ and $b$ are connected through a common parent variable $c$.
It is called tail-to-tail because the node $c$ has two outgoing edges (tails) pointing to $a$ and $b$.
\begin{figure}[H]
\centering
    \begin{tikzpicture}
    % Nodes
    \node[bn-node] (C) at (0, 1) {c};
    \node[bn-node] (A) at (-2, -1) {a};
    \node[bn-node] (B) at (2, -1) {b};
    % Edges
    \draw[bn-arrow] (C) -- (A);
    \draw[bn-arrow] (C) -- (B);
    \end{tikzpicture}
    \caption{Tail-to-tail connection}
\end{figure}
The joint distribution can be expressed as:
\begin{equation}
    \label{eq:tailToTail}
    P(a, b, c) = P(a | c) P(b| c) P(c)
\end{equation}
Given this, we want to understand the independency relationships between $a$ and $b$:
\begin{itemize}
    \item \textbf{Independent}: we need to check that $P(a, b) = P(a) P(b)$;
    To do this, we marginalize over $c$, applying the sum rule:
    \[ P(a, b) = \sum_{c} P(a, b, c) \]
    We can then substitute the joint distribution from eq \ref{eq:tailToTail}:
    \[ P(a, b) = \sum_{c} P(a | c) P(b| c) P(c) \neq P(a) P(b)\]
    So $a$ and $b$ are \textbf{not independent}.
    \item \textbf{Conditionally independent}: we need to check that $P(a, b | c) = P(a | c) P(b | c)$;
    We can apply the product rule:
    \[ P(a, b | c) = \frac{P(a, b, c)}{P(c)} \]
    We can then substitute the joint distribution from eq \ref{eq:tailToTail}:
    \[ P(a, b | c) = \frac{P(a | c) P(b| c) P(c)}{P(c)} = P(a | c) P(b | c) \]
    So $a$ and $b$ are \textbf{conditionally independent} given $c$.
\end{itemize}
We derive that, if $c$ is observed, $a$ and $b$ become independent.
\paragraph{Example:}
A classic example of tail-to-tail connection is the relationship between covid infection $c$, 
and two symptoms: fever $a$ and cough $b$.
If we observe the covid infection status, knowing whether a patient has a fever provides no 
additional information about whether they have a cough, and vice versa.
\\Variable $c$ is a common cause of both symptoms $a$ and $b$.

\subsubsection{Head-to-tail connection}
In a head-to-tail connection, variables $a$ is a parent of variable $c$, which is a parent of variable $b$.
It is called head-to-tail because $c$ has one incoming edge (head) from $a$ and one outgoing edge (tail) to $b$.
\begin{figure}[H]
\centering
    \begin{tikzpicture}
    % Nodes
    \node[bn-node] (A) at (-2, 0) {a};
    \node[bn-node] (C) at (0, 0) {c};
    \node[bn-node] (B) at (2, 0) {b};
    % Edges
    \draw[bn-arrow] (A) -- (C);
    \draw[bn-arrow] (C) -- (B);
    \end{tikzpicture}
    \caption{Head-to-tail connection}
\end{figure}
The joint distribution can be expressed as:
\[ p(a,b,c) = p(b|c) p(c|a) p(a) \]
Note that, using Bayes' theorem, we can rewrite $p(c|a) p(a)$ as $p(a|c) p(c)$, obtaining:
\[ p(a,b,c) = p(b|c) p(a|c) p(c) \]
We can note that this is exactly the same form as the tail-to-tail case \ref{eq:tailToTail}.
Even though it's trivial, we will repeat the independency analysis for clarity.
\begin{itemize}
    \item \textbf{Independent}: we need to check that $P(a, b) = P(a) P(b)$;
    \[ P(a, b) = \sum_{c} P(a, b, c) = \sum_{c} p(b|c) p(c|a) p(a) \neq P(a) P(b)\]
    So $a$ and $b$ are \textbf{not independent}.
    \item \textbf{Conditionally independent}: we need to check that $P(a, b | c) = P(a | c) P(b | c)$;
    \[ P(a, b | c) = \frac{P(a, b, c)}{P(c)} = \frac{p(b|c) p(c|a) p(a)}{P(c)} = P(a | c) P(b | c) \]
    So $a$ and $b$ are \textbf{conditionally independent} given $c$.
\end{itemize}
Intuitively, if we already know the value of $c$, knowing $a$ provides no additional information about $b$, and vice versa.
Differently from the tail-to-tail case, here $c$ is an intermediary variable between $a$ and $b$.
\paragraph{Example:}
A classic example of head-to-tail connection is the relationship between cloud coverage $a$, rain $c$, and getting wet $b$.
If we observe whether it is raining, knowing whether there is cloud coverage provides no additional 
information about wether we get wet. For both cloudy and clear skies, if it is raining we will get wet.

\subsubsection{Head-to-head connection}
In a head-to-head connection, variables $a$ and $b$ are both parents of variable $c$.
It is called head-to-head because $c$ has two incoming edges (heads) from $a$ and $b$.
\begin{figure}[H]
\centering
    \begin{tikzpicture}
    % Nodes
    \node[bn-node] (A) at (-2, 0) {a};
    \node[bn-node] (B) at (2, 0) {b};
    \node[bn-node] (C) at (0, -2) {c};
    % Edges
    \draw[bn-arrow] (A) -- (C);
    \draw[bn-arrow] (B) -- (C);
    \end{tikzpicture}
    \caption{Head-to-head connection}
\end{figure}
The joint distribution can be expressed as:
\[ p(a,b,c) = p(c|a,b) p(a) p(b) \]
As before, we want to understand the independency relationships between $a$ and $b$:
\begin{itemize}
    \item \textbf{Independent}:
    \[ P(a, b) = \sum_{c} P(a, b, c) = \sum_{c} p(c|a,b) p(a) p(b)\]
    Remember that summing over all possible values of $c$ gives 1, so:
    \[ P(a, b) = p(a) p(b) \]
    So $a$ and $b$ are \textbf{independent}.
    \item \textbf{Conditionally independent}:
    \[ P(a, b | c) = \frac{P(a, b, c)}{P(c)} = \frac{p(c|a,b) p(a) p(b)}{P(c)} \neq P(a | c) P(b | c) \]
    So $a$ and $b$ are \textbf{not conditionally independent} given $c$.
\end{itemize}
Intuitively, $a$ and $b$ are independent. Observing $c$ creates a dependency between $a$ and $b$.
This phenomenon is known as "explaining away": if we know that $c$ has occurred (both $a$ and $b$ could have caused it), 
observing one of the causes makes the other cause less likely.
\paragraph{Example:}
A classic example of head-to-head connection is the relationship between burglary $a$, earthquake $b$, and alarm ringing $c$.
Burglary and earthquake are independent events and are both causes of the alarm ringing.
However, if we know that the alarm has rung, observing a burglary makes an earthquake less likely.
\paragraph{General head-to-head case}
Let a \textit{descendant} of a node $x$ be any node that can be reached by following directed edges starting from $x$.
A head-to-head node $c$ unblocks the dependency path between it's parents $a$ and $b$ if either $c$ or any of its descendants are observed.
\\This can be better understood with an example:
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % Nodes
        \node[bn-node] (A) at (-1.5, 1) {a};
        \node[bn-node] (B) at (1.5, 1) {b};
        \node[bn-node] (C) at (0, 0) {c};
        \node[bn-node] (D) at (0, -1.2) {d};
        % Edges
        \draw[bn-arrow] (A) -- (C);
        \draw[bn-arrow] (B) -- (C);
        \draw[bn-arrow] (C) -- (D);
    \end{tikzpicture}
    \caption{General head-to-head case}
\end{figure}
In this case, if either $c$ or $d$ are observed, the dependency path between $a$ and $b$ is unblocked, making them dependent.
For instance, if we take the previous example of burglary, earthquake, and alarm ringing, and 
add a new variable $d$ representing a call from alarm company.
If we know either that the alarm has rung or that we received a call from the alarm company,
observing a burglary makes an earthquake less likely and vice versa.
\\This rule is valid for any descendant of a head-to-head node, but the further down the descendant is in the graph,
the weaker the dependency between the parents becomes.

\subsubsection{D-separation rules summary}
\begin{figure}[H]
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tikzpicture}[
        node distance=1cm,
        % Arrow Style
        arrow/.style={-{Stealth[scale=0.5]}, red, thick}
    ]
        % --- SECTION: NO EVIDENCE (TOP) ---
        \node (label_no_ev) at (-7.5, 2) {\textbf{no evidence}};

        % Tail to Tail (Top)
        \node[dse-node-empty] (c1) at (-4.5, 2.8) {\small $c$};
        \node[dse-node-empty] (a1) at (-5.5, 2) {\small $a$};
        \node[dse-node-empty] (b1) at (-3.5, 2) {\small $b$};
        \draw[arrow] (c1) -- (a1);
        \draw[arrow] (c1) -- (b1);

        % Head to Tail (Top)
        \node[dse-node-empty] (a2) at (-1.7, 2) {\small $a$};
        \node[dse-node-empty] (c2) at (-0.5, 2) {\small $c$};
        \node[dse-node-empty] (b2) at (0.7, 2) {\small $b$};
        \draw[arrow] (a2) -- (c2);
        \draw[arrow] (c2) -- (b2);

        % Head to Head (Top)
        \node[dse-node-empty] (a3) at (2.8, 2.8) {\small $a$};
        \node[dse-node-empty] (b3) at (4.2, 2.8) {\small $b$};
        \node[dse-node-empty] (c3) at (3.5, 2) {\small $c$};
        \draw[arrow] (a3) -- (c3);
        \draw[arrow] (b3) -- (c3);

        % --- SECTION: EVIDENCE (BOTTOM) ---
        \node (label_ev) at (-7.5, -2) {\textbf{evidence}};

        % Tail to Tail (Bottom)
        \node[dse-node-filled] (c4) at (-4.5, -1.2) {\small $c$};
        \node[dse-node-empty] (a4) at (-5.5, -2) {\small $a$};
        \node[dse-node-empty] (b4) at (-3.5, -2) {\small $b$};
        \draw[arrow] (c4) -- (a4);
        \draw[arrow] (c4) -- (b4);

        % Head to Tail (Bottom)
        \node[dse-node-empty] (a5) at (-1.7, -2) {\small $a$};
        \node[dse-node-filled] (c5) at (-0.5, -2) {\small $c$};
        \node[dse-node-empty] (b5) at (0.7, -2) {\small $b$};
        \draw[arrow] (a5) -- (c5);
        \draw[arrow] (c5) -- (b5);

        % Head to Head (Bottom)
        \node[dse-node-empty] (a6) at (2.8, -1.2) {\small $a$};
        \node[dse-node-empty] (b6) at (4.2, -1.2) {\small $b$};
        \node[dse-node-filled] (c6) at (3.5, -2) {\small $c$};
        \draw[arrow] (a6) -- (c6);
        \draw[arrow] (b6) -- (c6);

        % --- BACKGROUND LAYERS ---
        \begin{scope}[on background layer]
            % Large Horizontal Ellipses
            \node[dse-dep-bg] at (-2.4, 2.2) {};
            \node[dse-indep-bg] at (-2.4, -1.9) {};
            % Small Individual Ellipses (Right Side)
            \node[dse-small-indep] at (3.5, 2.4) {};
            \node[dse-small-dep] at (3.5, -1.6) {};
            % Vertical Dashed Outlines
            \node[dse-boundary, label={[label distance=10pt]above:tail to tail}] at (-4.5, 0.1) {};
            \node[dse-boundary, label={[label distance=10pt]above:head to tail}] at (-0.5, 0.1) {};
            \node[dse-boundary, label={[label distance=10pt]above:head to head}] at (3.5, 0.4) {};
        \end{scope}
        % --- LEGEND ---
        \node[draw, fill=yellow!25, minimum size=4mm, label=right:independent] at (-3, -4.5) {};
        \node[draw, fill=green!30, minimum size=4mm, label=right:dependent] at (1, -4.5) {};
    \end{tikzpicture}
    }
    \caption{D-separation rules summary}
\end{figure}

\subsubsection{General D-separation criterion}
\definition{D-separation}{
    \begin{itemize}
        \item Let $\mathcal{G}$ be a BN structure over a set of random variables $\boldsymbol{X}$;
        \item Given three disjoint subsets of variables $\boldsymbol{A}, \boldsymbol{B}, \boldsymbol{C} \subseteq \boldsymbol{X}$;
        \item We say that $\boldsymbol{A}$ and $\boldsymbol{B}$ are \textbf{D-separated} given $\boldsymbol{C}$ in $\mathcal{G}$ if
        all paths from any node in $\boldsymbol{A}$ to any node in $\boldsymbol{B}$ are blocked (no information flow possible).
        \item This can also be written as:
        \[ \text{D-sep}(\boldsymbol{A}, \boldsymbol{B} | \boldsymbol{C}) \]
        \end{itemize}
}
A path is considered blocked if it contains at least one node that meets one of the following conditions:
\begin{itemize}
    \item The arrows on the path meet either in a tail-to-tail or head-to-tail configuration, and the node is in $\boldsymbol{C}$ (observed);
    \item The arrows on the path meet in a head-to-head configuration, and neither the node nor any of its descendants are in $\boldsymbol{C}$ (not observed).
\end{itemize}
In other words, a path is blocked if the nodes $A$ and $B$ cannot influence each other given the observed variables in $\boldsymbol{C}$ ($A$ and $B$ are independent given $\boldsymbol{C}$).
If $\boldsymbol{A}$ and $\boldsymbol{B}$ are D-separated given $\boldsymbol{C}$, then they are conditionally independent given $\boldsymbol{C}$.

\paragraph{Examples:}
Consider the following BN structure:
\begin{figure}[H]
\centering
    \begin{tikzpicture}
    % Nodes
    \node[bn-node] (A) at (-1.5, 1) {A};
    \node[bn-node] (F) at (1.5, 1) {F};
    \node[bn-node] (E) at (0, 0) {E};
    \node[bn-node] (B) at (3, 0) {B};
    \node[bn-node] (C) at (0, -1) {C};
    % Edges
    \draw[bn-arrow] (A) -- (E);
    \draw[bn-arrow] (F) -- (E);
    \draw[bn-arrow] (E) -- (C);
    \draw[bn-arrow] (F) -- (B);
    \end{tikzpicture}
    \caption{Example of Bayesian Network structure for D-separation}
\end{figure}
Consider the two following examples:\
\begin{enumerate}
    \item Are $A$ and $B$ D-separated given $C$?
    \begin{itemize}
        \item Node $F$ is in a tail to tail connection and is not observed. This implies that the path is not blocked;
        \item Node $E$ is in a head to head connection and is not observed. One of its child ($C$) is observed, meaning
        that they are dependent (the path is not blocked);
        \item Therefore, there exists an unblocked path from $A$ to $B$, so they are \textbf{not D-separated} given $C$.
    \end{itemize}
    \item Are $A$ and $B$ D-separated given $F$?
    \begin{itemize}
        \item Node $F$ is in a tail to tail connection and is observed. This implies that the path is blocked;
        \item Therefore, all paths from $A$ to $B$ are blocked, so they are \textbf{D-separated} given $F$.
    \end{itemize}
\end{enumerate}

\subsection{Bayesian Network Independencies}
A Bayesian Network structure $\mathcal{G}$ encodes both a set of local independencies $\mathcal{I}_l(\mathcal{G})$ (see eq \ref{eq:localIndependencies})
and a set of global independencies $\mathcal{I}(\mathcal{G})$.
Global independencies are all the independencies that can be derived using the D-separation criterion.
We can formally define them as:
\begin{figure}[H]
\definition{Global independencies}{
    \[ \mathcal{I}(\mathcal{G}) = \{ (A \perp B| C): \text{D-sep}(A, B| C) \} \]
}
\end{figure}

\subsection{Equivalence Classes}
Different BN structures can represent the same set of independencies assumptions.
\definition{I-Equivalence}{
    We say that two BN structures $\mathcal{G}, \mathcal{G'}$ are \textbf{I-equivalent} if they encode the same set of independencies:
    \[ \mathcal{I}(\mathcal{G}) = \mathcal{I}(\mathcal{G'}) \]
}
As we have previously seen, for a structure $\mathcal{G}$ to be an I-map for a distribution $p$ it's not
necessary that $\mathcal{I}(\mathcal{G}) = \mathcal{I}(p)$ (doesn't need to encode all independencies in $p$).
\\\\As a consequence, also a fully connected graph (no independencies) is an I-map for any distribution $p$ over the same set of variables.
Such graph is obviously not very useful, as it doesn't provide any information about independencies in $p$.
\\To overcome this, we can define a \textbf{minimal I-map}.
\definition{Minimal I-Map}{
    A BN structure $\mathcal{G}$ is a \textbf{minimal I-map} for a distribution $p$ if:
    \begin{itemize}
        \item $\mathcal{G}$ is an I-map for $p$;
        \item Removing any edge from $\mathcal{G}$ would result in a graph that is not an I-map for $p$.
    \end{itemize}
}
This means it's impossible to remove any edge from $\mathcal{G}$ without introducing independencies that don't hold in $p$.
Note that a minimal I-map doesn't necessarily encode all independencies in $p$. 
For this, we need a stronger definition, namely the \textbf{perfect I-map}.

\definition{Perfect I-Map}{
    We say that a BN structure $\mathcal{G}$ is a \textbf{perfect I-map} for a distribution $p$ iff it captures
    all and only the independencies that hold in $p$:
    \[ \mathcal{I}(\mathcal{G}) = \mathcal{I}(p) \]
}
Unfortunately, not all distributions $p$ admit a perfect I-map, some cannot be perfectly represented by any BN structure.
\subsection{Building Bayesian Networks}
Here are a list of practical guidelines to build a BN structure:
\begin{itemize}
    \item Get together with domain experts. This allow to gather prior knowledge about the relationships between variables;
    \item Define variables for both entities that are directly observable and that you are interested in predicting. 
    Also, including latent variables can help to better represent the dependencies in the data;
    \item For adding edges, follow causal relationships when possible. 
    For example, if variable $A$ causes variable $B$, add a directed edge from $A$ to $B$;
    \item Avoid assigning zero probabilities in the variables distributions.
    \item If data is available, we can use structure learning algorithms to learn the BN structure from data.
\end{itemize}